callbacks:
  patience: 100
  tol: 0
data:
  baseMVA: 100
  learn_mask: false
  mask_dim: 6
  mask_ratio: 0.5
  mask_type: rnd
  mask_value: 0.0
  networks:
  - case240_pserc
  - case24_ieee_rts
  - case30_ieee
  - case57_ieee
  - case89_pegase
  - case118_ieee
  normalization: baseMVAnorm
  scenarios:
  - 100000
  - 100000
  - 100000
  - 100000
  - 100000
  - 100000
  test_ratio: 0.1
  val_ratio: 0.1
  workers: 4
model:
  attention_head: 8
  dropout: 0.1
  edge_dim: 2
  hidden_size: 256
  input_dim: 9
  num_layers: 8
  output_dim: 6
  pe_dim: 20
  type: GPSTransformer
optimizer:
  beta1: 0.9
  beta2: 0.999
  learning_rate: 0.0001
  lr_decay: 0.7
  lr_patience: 10
seed: 0
training:
  batch_size: 64
  epochs: 500
  loss_weights:
  - 0.01
  - 0.99
  losses:
  - MaskedMSE
  - PBE
  accelerator: auto
  devices: auto
  strategy: auto
