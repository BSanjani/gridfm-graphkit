callbacks:
  patience: 100
  tol: 0
data:
  baseMVA: 100
  learn_mask: false
  mask_dim: 6
  mask_ratio: 0.5
  mask_type: pf
  mask_value: 0.0
  networks:
  - case30_ieee
  normalization: baseMVAnorm
  scenarios:
  - 270000
  test_ratio: 0.1
  val_ratio: 0.1
  workers: 16
model:
  attention_head: 8
  edge_dim: 2
  hidden_size: 80
  input_dim: 9
  num_layers: 12
  output_dim: 6
  pe_dim: 20
  type: GNN_PBE_TransformerConv
optimizer:
  beta1: 0.9
  beta2: 0.999
  learning_rate: 0.0001
  lr_decay: 0.7
  lr_patience: 5
seed: 0
training:
  batch_size: 32
  epochs: 150
  loss_weights:
  - 0.9
  - 0.1
  - 0.0
  losses:
  - LayeredWeightedPhysicsLoss
  - MaskedMSE
  - PBE
  base_weight: 0.6
  accelerator: auto
  devices: auto
  strategy: auto
verbose: false
