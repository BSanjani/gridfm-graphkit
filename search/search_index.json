{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"gridfm-graphkit","text":"<p>This library is brought to you by the GridFM team to train, finetune and interact with a foundation model for the electric power grid.</p>"},{"location":"#citation-tbd","title":"Citation: TBD","text":""},{"location":"datasets/data_modules/","title":"LitGridDataModule","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>PyTorch Lightning DataModule for power grid datasets.</p> <p>This datamodule handles loading, preprocessing, splitting, and batching of power grid graph datasets (<code>GridDatasetDisk</code>) for training, validation, testing, and prediction. It ensures reproducibility through fixed seeds.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>NestedNamespace</code> <p>Experiment configuration.</p> required <code>data_dir</code> <code>str</code> <p>Root directory for datasets. Defaults to \"./data\".</p> <code>'./data'</code> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>Batch size for all dataloaders. From <code>args.training.batch_size</code></p> <code>node_normalizers</code> <code>list</code> <p>List of node feature normalizers, one per dataset.</p> <code>edge_normalizers</code> <code>list</code> <p>List of edge feature normalizers, one per dataset.</p> <code>datasets</code> <code>list</code> <p>Original datasets for each network.</p> <code>train_datasets</code> <code>list</code> <p>Train splits for each network.</p> <code>val_datasets</code> <code>list</code> <p>Validation splits for each network.</p> <code>test_datasets</code> <code>list</code> <p>Test splits for each network.</p> <code>train_dataset_multi</code> <code>ConcatDataset</code> <p>Concatenated train datasets for multi-network training.</p> <code>val_dataset_multi</code> <code>ConcatDataset</code> <p>Concatenated validation datasets for multi-network validation.</p> <code>_is_setup_done</code> <code>bool</code> <p>Tracks whether <code>setup</code> has been executed to avoid repeated processing.</p> <p>Methods:</p> Name Description <code>setup</code> <p>Load and preprocess datasets, split into train/val/test, and store normalizers. Handles distributed preprocessing safely.</p> <code>train_dataloader</code> <p>Returns a DataLoader for concatenated training datasets.</p> <code>val_dataloader</code> <p>Returns a DataLoader for concatenated validation datasets.</p> <code>test_dataloader</code> <p>Returns a list of DataLoaders, one per test dataset.</p> <code>predict_dataloader</code> <p>Returns a list of DataLoaders, one per test dataset for prediction.</p> Notes <ul> <li>Preprocessing is only performed on rank 0 in distributed settings.</li> <li>Subsets and splits are deterministic based on the provided random seed.</li> <li>Normalizers are loaded for each network independently.</li> <li>Test and predict dataloaders are returned as lists, one per dataset.</li> </ul> Example <pre><code>from gridfm_graphkit.datasets.powergrid_datamodule import LitGridDataModule\nfrom gridfm_graphkit.io.param_handler import NestedNamespace\nimport yaml\n\nwith open(\"config/config.yaml\") as f:\n    base_config = yaml.safe_load(f)\nargs = NestedNamespace(**base_config)\n\ndatamodule = LitGridDataModule(args, data_dir=\"./data\")\n\ndatamodule.setup(\"fit\")\ntrain_loader = datamodule.train_dataloader()\n</code></pre> Source code in <code>gridfm_graphkit/datasets/powergrid_datamodule.py</code> <pre><code>class LitGridDataModule(L.LightningDataModule):\n    \"\"\"\n    PyTorch Lightning DataModule for power grid datasets.\n\n    This datamodule handles loading, preprocessing, splitting, and batching\n    of power grid graph datasets (`GridDatasetDisk`) for training, validation,\n    testing, and prediction. It ensures reproducibility through fixed seeds.\n\n    Args:\n        args (NestedNamespace): Experiment configuration.\n        data_dir (str, optional): Root directory for datasets. Defaults to \"./data\".\n\n    Attributes:\n        batch_size (int): Batch size for all dataloaders. From ``args.training.batch_size``\n        node_normalizers (list): List of node feature normalizers, one per dataset.\n        edge_normalizers (list): List of edge feature normalizers, one per dataset.\n        datasets (list): Original datasets for each network.\n        train_datasets (list): Train splits for each network.\n        val_datasets (list): Validation splits for each network.\n        test_datasets (list): Test splits for each network.\n        train_dataset_multi (ConcatDataset): Concatenated train datasets for multi-network training.\n        val_dataset_multi (ConcatDataset): Concatenated validation datasets for multi-network validation.\n        _is_setup_done (bool): Tracks whether `setup` has been executed to avoid repeated processing.\n\n    Methods:\n        setup(stage):\n            Load and preprocess datasets, split into train/val/test, and store normalizers.\n            Handles distributed preprocessing safely.\n        train_dataloader():\n            Returns a DataLoader for concatenated training datasets.\n        val_dataloader():\n            Returns a DataLoader for concatenated validation datasets.\n        test_dataloader():\n            Returns a list of DataLoaders, one per test dataset.\n        predict_dataloader():\n            Returns a list of DataLoaders, one per test dataset for prediction.\n\n    Notes:\n        - Preprocessing is only performed on rank 0 in distributed settings.\n        - Subsets and splits are deterministic based on the provided random seed.\n        - Normalizers are loaded for each network independently.\n        - Test and predict dataloaders are returned as lists, one per dataset.\n\n    Example:\n        ```python\n        from gridfm_graphkit.datasets.powergrid_datamodule import LitGridDataModule\n        from gridfm_graphkit.io.param_handler import NestedNamespace\n        import yaml\n\n        with open(\"config/config.yaml\") as f:\n            base_config = yaml.safe_load(f)\n        args = NestedNamespace(**base_config)\n\n        datamodule = LitGridDataModule(args, data_dir=\"./data\")\n\n        datamodule.setup(\"fit\")\n        train_loader = datamodule.train_dataloader()\n        ```\n    \"\"\"\n\n    def __init__(self, args: NestedNamespace, data_dir: str = \"./data\"):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = int(args.training.batch_size)\n        self.args = args\n        self.node_normalizers = []\n        self.edge_normalizers = []\n        self.datasets = []\n        self.train_datasets = []\n        self.val_datasets = []\n        self.test_datasets = []\n        self._is_setup_done = False\n\n    def setup(self, stage: str):\n        if self._is_setup_done:\n            print(f\"Setup already done for stage={stage}, skipping...\")\n            return\n\n        for i, network in enumerate(self.args.data.networks):\n            node_normalizer, edge_normalizer = load_normalizer(args=self.args)\n            self.node_normalizers.append(node_normalizer)\n            self.edge_normalizers.append(edge_normalizer)\n\n            # Create torch dataset and split\n            data_path_network = os.path.join(self.data_dir, network)\n\n            # Run preprocessing only on rank 0\n            if dist.is_available() and dist.is_initialized() and dist.get_rank() == 0:\n                print(f\"Pre-processing of {network} dataset on rank 0\")\n                _ = GridDatasetDisk(  # just to trigger processing\n                    root=data_path_network,\n                    norm_method=self.args.data.normalization,\n                    node_normalizer=node_normalizer,\n                    edge_normalizer=edge_normalizer,\n                    pe_dim=self.args.model.pe_dim,\n                    mask_dim=self.args.data.mask_dim,\n                    transform=get_transform(args=self.args),\n                )\n\n            # All ranks wait here until processing is done\n            if torch.distributed.is_available() and torch.distributed.is_initialized():\n                torch.distributed.barrier()\n\n            dataset = GridDatasetDisk(\n                root=data_path_network,\n                norm_method=self.args.data.normalization,\n                node_normalizer=node_normalizer,\n                edge_normalizer=edge_normalizer,\n                pe_dim=self.args.model.pe_dim,\n                mask_dim=self.args.data.mask_dim,\n                transform=get_transform(args=self.args),\n            )\n            self.datasets.append(dataset)\n\n            num_scenarios = self.args.data.scenarios[i]\n            if num_scenarios &gt; len(dataset):\n                warnings.warn(\n                    f\"Requested number of scenarios ({num_scenarios}) exceeds dataset size ({len(dataset)}). \"\n                    \"Using the full dataset instead.\",\n                )\n                num_scenarios = len(dataset)\n\n            # Create a subset\n            all_indices = list(range(len(dataset)))\n            # Random seed set before every shuffle for reproducibility in case the power grid datasets are analyzed in a different order\n            random.seed(self.args.seed)\n            random.shuffle(all_indices)\n            subset_indices = all_indices[:num_scenarios]\n            dataset = Subset(dataset, subset_indices)\n\n            # Random seed set before every split, same as above\n            np.random.seed(self.args.seed)\n            train_dataset, val_dataset, test_dataset = split_dataset(\n                dataset,\n                self.data_dir,\n                self.args.data.val_ratio,\n                self.args.data.test_ratio,\n            )\n\n            self.train_datasets.append(train_dataset)\n            self.val_datasets.append(val_dataset)\n            self.test_datasets.append(test_dataset)\n\n        self.train_dataset_multi = ConcatDataset(self.train_datasets)\n        self.val_dataset_multi = ConcatDataset(self.val_datasets)\n        self._is_setup_done = True\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset_multi,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=self.args.data.workers,\n            pin_memory=True,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset_multi,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.args.data.workers,\n            pin_memory=True,\n        )\n\n    def test_dataloader(self):\n        return [\n            DataLoader(\n                i,\n                batch_size=self.batch_size,\n                shuffle=False,\n                num_workers=self.args.data.workers,\n                pin_memory=True,\n            )\n            for i in self.test_datasets\n        ]\n\n    def predict_dataloader(self):\n        return [\n            DataLoader(\n                i,\n                batch_size=self.batch_size,\n                shuffle=False,\n                num_workers=self.args.data.workers,\n                pin_memory=True,\n            )\n            for i in self.test_datasets\n        ]\n</code></pre>"},{"location":"datasets/data_normalization/","title":"Data Normalization","text":"<p>Normalization improves neural network training by ensuring features are well-scaled, preventing issues like exploding gradients and slow convergence. In power grids, where variables like voltage and power span wide ranges, normalization is essential. The <code>gridfm-graphkit</code> package offers four methods:</p> <ul> <li><code>Min-Max Normalization</code></li> <li><code>Standardization (Z-score)</code></li> <li><code>Identity (no normalization)</code></li> <li><code>BaseMVA Normalization</code></li> </ul> <p>Each of these strategies implements a unified interface and can be used interchangeably depending on the learning task and data characteristics.</p> <p>Users can create their own custom normalizers by extending the base <code>Normalizer</code> class to suit specific needs.</p>"},{"location":"datasets/data_normalization/#available-normalizers","title":"Available Normalizers","text":""},{"location":"datasets/data_normalization/#normalizer","title":"<code>Normalizer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all normalization strategies.</p> Source code in <code>gridfm_graphkit/datasets/normalizers.py</code> <pre><code>class Normalizer(ABC):\n    \"\"\"\n    Abstract base class for all normalization strategies.\n    \"\"\"\n\n    @abstractmethod\n    def fit(self, data: torch.Tensor) -&gt; dict:\n        \"\"\"\n        Fit normalization parameters from data.\n\n        Args:\n            data: Input tensor.\n\n        Returns:\n            Dictionary of computed parameters.\n        \"\"\"\n\n    @abstractmethod\n    def fit_from_dict(self, params: dict):\n        \"\"\"\n        Set parameters from a precomputed dictionary.\n\n        Args:\n            params: Dictionary of parameters.\n        \"\"\"\n\n    @abstractmethod\n    def transform(self, data: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Normalize the input data.\n\n        Args:\n            data: Input tensor.\n\n        Returns:\n            Normalized tensor.\n        \"\"\"\n\n    @abstractmethod\n    def inverse_transform(self, normalized_data: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Undo normalization.\n\n        Args:\n            normalized_data: Normalized tensor.\n\n        Returns:\n            Original tensor.\n        \"\"\"\n\n    @abstractmethod\n    def get_stats(self) -&gt; dict:\n        \"\"\"\n        Return the stored normalization statistics for logging/inspection.\n        \"\"\"\n</code></pre> <code>fit(data)</code> <code>abstractmethod</code> \u00b6 <p>Fit normalization parameters from data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of computed parameters.</p> Source code in <code>gridfm_graphkit/datasets/normalizers.py</code> <pre><code>@abstractmethod\ndef fit(self, data: torch.Tensor) -&gt; dict:\n    \"\"\"\n    Fit normalization parameters from data.\n\n    Args:\n        data: Input tensor.\n\n    Returns:\n        Dictionary of computed parameters.\n    \"\"\"\n</code></pre> <code>fit_from_dict(params)</code> <code>abstractmethod</code> \u00b6 <p>Set parameters from a precomputed dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Dictionary of parameters.</p> required Source code in <code>gridfm_graphkit/datasets/normalizers.py</code> <pre><code>@abstractmethod\ndef fit_from_dict(self, params: dict):\n    \"\"\"\n    Set parameters from a precomputed dictionary.\n\n    Args:\n        params: Dictionary of parameters.\n    \"\"\"\n</code></pre> <code>get_stats()</code> <code>abstractmethod</code> \u00b6 <p>Return the stored normalization statistics for logging/inspection.</p> Source code in <code>gridfm_graphkit/datasets/normalizers.py</code> <pre><code>@abstractmethod\ndef get_stats(self) -&gt; dict:\n    \"\"\"\n    Return the stored normalization statistics for logging/inspection.\n    \"\"\"\n</code></pre> <code>inverse_transform(normalized_data)</code> <code>abstractmethod</code> \u00b6 <p>Undo normalization.</p> <p>Parameters:</p> Name Type Description Default <code>normalized_data</code> <code>Tensor</code> <p>Normalized tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Original tensor.</p> Source code in <code>gridfm_graphkit/datasets/normalizers.py</code> <pre><code>@abstractmethod\ndef inverse_transform(self, normalized_data: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Undo normalization.\n\n    Args:\n        normalized_data: Normalized tensor.\n\n    Returns:\n        Original tensor.\n    \"\"\"\n</code></pre> <code>transform(data)</code> <code>abstractmethod</code> \u00b6 <p>Normalize the input data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor.</p> Source code in <code>gridfm_graphkit/datasets/normalizers.py</code> <pre><code>@abstractmethod\ndef transform(self, data: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Normalize the input data.\n\n    Args:\n        data: Input tensor.\n\n    Returns:\n        Normalized tensor.\n    \"\"\"\n</code></pre>"},{"location":"datasets/data_normalization/#minmaxnormalizer","title":"<code>MinMaxNormalizer</code>","text":"<p>               Bases: <code>Normalizer</code></p> <p>Scales each feature to the [0, 1] range.</p> <p>Parameters:</p> Name Type Description Default <code>node_data</code> <code>bool</code> <p>Whether data is node-level or edge-level</p> required <code>args</code> <code>NestedNamespace</code> <p>Parameters</p> required Source code in <code>gridfm_graphkit/datasets/normalizers.py</code> <pre><code>@NORMALIZERS_REGISTRY.register(\"minmax\")\nclass MinMaxNormalizer(Normalizer):\n    \"\"\"\n    Scales each feature to the [0, 1] range.\n\n    Args:\n        node_data (bool): Whether data is node-level or edge-level\n        args (NestedNamespace): Parameters\n\n    \"\"\"\n\n    def __init__(self, node_data: bool, args):\n        self.min_val = None\n        self.max_val = None\n\n    def to(self, device):\n        self.min_val = self.min_val.to(device)\n        self.max_val = self.max_val.to(device)\n\n    def fit(self, data: torch.Tensor) -&gt; dict:\n        self.min_val, _ = data.min(axis=0)\n        self.max_val, _ = data.max(axis=0)\n\n        return {\"min_value\": self.min_val, \"max_value\": self.max_val}\n\n    def fit_from_dict(self, params: dict):\n        if self.min_val is None:\n            self.min_val = params.get(\"min_value\")\n        if self.max_val is None:\n            self.max_val = params.get(\"max_value\")\n\n    def transform(self, data: torch.Tensor) -&gt; torch.Tensor:\n        if self.min_val is None or self.max_val is None:\n            raise ValueError(\"fit must be called before transform.\")\n\n        diff = self.max_val - self.min_val\n        diff[diff == 0] = 1  # Avoid division by zero for features with zero range\n        return (data - self.min_val) / diff\n\n    def inverse_transform(self, normalized_data: torch.Tensor) -&gt; torch.Tensor:\n        if self.min_val is None or self.max_val is None:\n            raise ValueError(\"fit must be called before inverse_transform.\")\n\n        diff = self.max_val - self.min_val\n        diff[diff == 0] = 1\n        return (normalized_data * diff) + self.min_val\n\n    def get_stats(self) -&gt; dict:\n        return {\n            \"min_value\": self.min_val.tolist() if self.min_val is not None else None,\n            \"max_value\": self.max_val.tolist() if self.max_val is not None else None,\n        }\n</code></pre>"},{"location":"datasets/data_normalization/#standardizer","title":"<code>Standardizer</code>","text":"<p>               Bases: <code>Normalizer</code></p> <p>Standardizes each feature to zero mean and unit variance.</p> <p>Parameters:</p> Name Type Description Default <code>node_data</code> <code>bool</code> <p>Whether data is node-level or edge-level</p> required <code>args</code> <code>NestedNamespace</code> <p>Parameters</p> required Source code in <code>gridfm_graphkit/datasets/normalizers.py</code> <pre><code>@NORMALIZERS_REGISTRY.register(\"standard\")\nclass Standardizer(Normalizer):\n    \"\"\"\n    Standardizes each feature to zero mean and unit variance.\n\n    Args:\n        node_data (bool): Whether data is node-level or edge-level\n        args (NestedNamespace): Parameters\n\n    \"\"\"\n\n    def __init__(self, node_data: bool, args):\n        self.mean = None\n        self.std = None\n\n    def to(self, device):\n        self.mean = self.mean.to(device)\n        self.std = self.std.to(device)\n\n    def fit(self, data: torch.Tensor) -&gt; dict:\n        self.mean = data.mean(axis=0)\n        self.std = data.std(axis=0)\n\n        return {\"mean_value\": self.mean, \"std_value\": self.std}\n\n    def fit_from_dict(self, params: dict):\n        if self.mean is None:\n            self.mean = params.get(\"mean_value\")\n        if self.std is None:\n            self.std = params.get(\"std_value\")\n\n    def transform(self, data: torch.Tensor) -&gt; torch.Tensor:\n        if self.mean is None or self.std is None:\n            raise ValueError(\"fit must be called before transform.\")\n\n        std = self.std.clone()\n        std[std == 0] = 1  # Avoid division by zero for features with zero std\n        return (data - self.mean) / std\n\n    def inverse_transform(self, normalized_data: torch.Tensor) -&gt; torch.Tensor:\n        if self.mean is None or self.std is None:\n            raise ValueError(\"fit must be called before inverse_transform.\")\n\n        std = self.std.clone()\n        std[std == 0] = 1\n        return (normalized_data * std) + self.mean\n\n    def get_stats(self) -&gt; dict:\n        return {\n            \"mean\": self.mean.tolist() if self.mean is not None else None,\n            \"std\": self.std.tolist() if self.std is not None else None,\n        }\n</code></pre>"},{"location":"datasets/data_normalization/#basemvanormalizer","title":"<code>BaseMVANormalizer</code>","text":"<p>               Bases: <code>Normalizer</code></p> <p>In power systems, a suitable normalization strategy must preserve the physical properties of the system. A known method is the conversion to the per-unit (p.u.) system, which expresses electrical quantities such as voltage, current, power, and impedance as fractions of predefined base values. These base values are usually chosen based on system parameters, such as rated voltage. The per-unit conversion ensures that power system equations remain scale-invariant, preserving fundamental physical relationships.</p> Source code in <code>gridfm_graphkit/datasets/normalizers.py</code> <pre><code>@NORMALIZERS_REGISTRY.register(\"baseMVAnorm\")\nclass BaseMVANormalizer(Normalizer):\n    \"\"\"\n    In power systems, a suitable normalization strategy must preserve the physical properties of\n    the system. A known method is the conversion to the per-unit (p.u.) system, which expresses\n    electrical quantities such as voltage, current, power, and impedance as fractions of predefined\n    base values. These base values are usually chosen based on system parameters, such as rated\n    voltage. The per-unit conversion ensures that power system equations remain scale-invariant,\n    preserving fundamental physical relationships.\n    \"\"\"\n\n    def __init__(self, node_data: bool, args):\n        \"\"\"\n        Args:\n            node_data: Whether data is node-level or edge-level\n            args (NestedNamespace): Parameters\n\n        Attributes:\n            baseMVA (float): baseMVA found in casefile. From ``args.data.baseMVA``.\n        \"\"\"\n        self.node_data = node_data\n        self.baseMVA_orig = getattr(args.data, \"baseMVA\", 100)\n        self.baseMVA = None\n\n    def to(self, device):\n        pass\n\n    def fit(self, data: torch.Tensor, baseMVA: float = None) -&gt; dict:\n        if self.node_data:\n            self.baseMVA = data[:, [PD, QD, PG, QG]].max()\n        else:\n            self.baseMVA = baseMVA\n\n        return {\"baseMVA_orig\": self.baseMVA_orig, \"baseMVA\": self.baseMVA}\n\n    def fit_from_dict(self, params: dict):\n        if self.baseMVA is None:\n            self.baseMVA = params.get(\"baseMVA\")\n        if self.baseMVA_orig is None:\n            self.baseMVA_orig = params.get(\"baseMVA_orig\")\n\n    def transform(self, data: torch.Tensor) -&gt; torch.Tensor:\n        if self.baseMVA is None:\n            raise ValueError(\"BaseMVA is not specified\")\n\n        if self.baseMVA == 0:\n            raise ZeroDivisionError(\"BaseMVA is 0.\")\n\n        if self.node_data:\n            data[:, PD] = data[:, PD] / self.baseMVA\n            data[:, QD] = data[:, QD] / self.baseMVA\n            data[:, PG] = data[:, PG] / self.baseMVA\n            data[:, QG] = data[:, QG] / self.baseMVA\n            data[:, VA] = data[:, VA] * torch.pi / 180.0\n        else:\n            data = data * self.baseMVA_orig / self.baseMVA\n\n        return data\n\n    def inverse_transform(self, normalized_data: torch.Tensor) -&gt; torch.Tensor:\n        if self.baseMVA is None:\n            raise ValueError(\"fit must be called before inverse_transform.\")\n\n        if self.node_data:\n            normalized_data[:, PD] = normalized_data[:, PD] * self.baseMVA\n            normalized_data[:, QD] = normalized_data[:, QD] * self.baseMVA\n            normalized_data[:, PG] = normalized_data[:, PG] * self.baseMVA\n            normalized_data[:, QG] = normalized_data[:, QG] * self.baseMVA\n            normalized_data[:, VA] = normalized_data[:, VA] * 180.0 / torch.pi\n        else:\n            normalized_data = normalized_data * self.baseMVA / self.baseMVA_orig\n\n        return normalized_data\n\n    def get_stats(self) -&gt; dict:\n        return {\n            \"baseMVA\": self.baseMVA,\n            \"baseMVA_orig\": self.baseMVA_orig,\n        }\n</code></pre> <code>__init__(node_data, args)</code> \u00b6 <p>Parameters:</p> Name Type Description Default <code>node_data</code> <code>bool</code> <p>Whether data is node-level or edge-level</p> required <code>args</code> <code>NestedNamespace</code> <p>Parameters</p> required <p>Attributes:</p> Name Type Description <code>baseMVA</code> <code>float</code> <p>baseMVA found in casefile. From <code>args.data.baseMVA</code>.</p> Source code in <code>gridfm_graphkit/datasets/normalizers.py</code> <pre><code>def __init__(self, node_data: bool, args):\n    \"\"\"\n    Args:\n        node_data: Whether data is node-level or edge-level\n        args (NestedNamespace): Parameters\n\n    Attributes:\n        baseMVA (float): baseMVA found in casefile. From ``args.data.baseMVA``.\n    \"\"\"\n    self.node_data = node_data\n    self.baseMVA_orig = getattr(args.data, \"baseMVA\", 100)\n    self.baseMVA = None\n</code></pre>"},{"location":"datasets/data_normalization/#identitynormalizer","title":"<code>IdentityNormalizer</code>","text":"<p>               Bases: <code>Normalizer</code></p> <p>No normalization: returns data unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>node_data</code> <code>bool</code> <p>Whether data is node-level or edge-level</p> required <code>args</code> <code>NestedNamespace</code> <p>Parameters</p> required Source code in <code>gridfm_graphkit/datasets/normalizers.py</code> <pre><code>@NORMALIZERS_REGISTRY.register(\"identity\")\nclass IdentityNormalizer(Normalizer):\n    \"\"\"\n    No normalization: returns data unchanged.\n\n    Args:\n            node_data: Whether data is node-level or edge-level\n            args (NestedNamespace): Parameters\n    \"\"\"\n\n    def __init__(self, node_data: bool, args):\n        pass\n\n    def fit(self, data: torch.Tensor) -&gt; dict:\n        return {}\n\n    def fit_from_dict(self, params: dict):\n        pass\n\n    def transform(self, data: torch.Tensor) -&gt; torch.Tensor:\n        return data\n\n    def inverse_transform(self, normalized_data: torch.Tensor) -&gt; torch.Tensor:\n        return normalized_data\n\n    def get_stats(self) -&gt; dict:\n        return {\"note\": \"No normalization applied.\"}\n</code></pre>"},{"location":"datasets/data_normalization/#usage-workflow","title":"Usage Workflow","text":"<p>Example:</p> <pre><code>from gridfm_graphkit.datasets.normalizers import MinMaxNormalizer\nimport torch\n\ndata = torch.randn(100, 5)  # Example tensor\n\nnormalizer = MinMaxNormalizer(node_data=True,args=None)\nparams = normalizer.fit(data)\nnormalized = normalizer.transform(data)\nrestored = normalizer.inverse_transform(normalized)\n</code></pre>"},{"location":"datasets/powergrid/","title":"Power Grid datasets","text":""},{"location":"datasets/powergrid/#griddatasetdisk","title":"<code>GridDatasetDisk</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A PyTorch Geometric <code>Dataset</code> for power grid data stored on disk. This dataset reads node and edge CSV files, applies normalization, and saves each graph separately on disk as a processed file. Data is loaded from disk lazily on demand.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory where the dataset is stored.</p> required <code>norm_method</code> <code>str</code> <p>Identifier for normalization method (e.g., \"minmax\", \"standard\").</p> required <code>node_normalizer</code> <code>Normalizer</code> <p>Normalizer used for node features.</p> required <code>edge_normalizer</code> <code>Normalizer</code> <p>Normalizer used for edge features.</p> required <code>pe_dim</code> <code>int</code> <p>Length of the random walk used for positional encoding.</p> required <code>mask_dim</code> <code>int</code> <p>Number of features per-node that could be masked.</p> <code>6</code> <code>transform</code> <code>callable</code> <p>Transformation applied at runtime.</p> <code>None</code> <code>pre_transform</code> <code>callable</code> <p>Transformation applied before saving to disk.</p> <code>None</code> <code>pre_filter</code> <code>callable</code> <p>Filter to determine which graphs to keep.</p> <code>None</code> Source code in <code>gridfm_graphkit/datasets/powergrid_dataset.py</code> <pre><code>class GridDatasetDisk(Dataset):\n    \"\"\"\n    A PyTorch Geometric `Dataset` for power grid data stored on disk.\n    This dataset reads node and edge CSV files, applies normalization,\n    and saves each graph separately on disk as a processed file.\n    Data is loaded from disk lazily on demand.\n\n    Args:\n        root (str): Root directory where the dataset is stored.\n        norm_method (str): Identifier for normalization method (e.g., \"minmax\", \"standard\").\n        node_normalizer (Normalizer): Normalizer used for node features.\n        edge_normalizer (Normalizer): Normalizer used for edge features.\n        pe_dim (int): Length of the random walk used for positional encoding.\n        mask_dim (int, optional): Number of features per-node that could be masked.\n        transform (callable, optional): Transformation applied at runtime.\n        pre_transform (callable, optional): Transformation applied before saving to disk.\n        pre_filter (callable, optional): Filter to determine which graphs to keep.\n    \"\"\"\n\n    def __init__(\n        self,\n        root: str,\n        norm_method: str,\n        node_normalizer: Normalizer,\n        edge_normalizer: Normalizer,\n        pe_dim: int,\n        mask_dim: int = 6,\n        transform: Optional[Callable] = None,\n        pre_transform: Optional[Callable] = None,\n        pre_filter: Optional[Callable] = None,\n    ):\n        self.norm_method = norm_method\n        self.node_normalizer = node_normalizer\n        self.edge_normalizer = edge_normalizer\n        self.pe_dim = pe_dim\n        self.mask_dim = mask_dim\n        self.length = None\n\n        super().__init__(root, transform, pre_transform, pre_filter)\n\n        # Load normalization stats if available\n        node_stats_path = osp.join(\n            self.processed_dir,\n            f\"node_stats_{self.norm_method}.pt\",\n        )\n        edge_stats_path = osp.join(\n            self.processed_dir,\n            f\"edge_stats_{self.norm_method}.pt\",\n        )\n        if osp.exists(node_stats_path) and osp.exists(edge_stats_path):\n            self.node_stats = torch.load(node_stats_path, weights_only=False)\n            self.edge_stats = torch.load(edge_stats_path, weights_only=False)\n            self.node_normalizer.fit_from_dict(self.node_stats)\n            self.edge_normalizer.fit_from_dict(self.edge_stats)\n\n    @property\n    def raw_file_names(self):\n        return [\"pf_node.csv\", \"pf_edge.csv\"]\n\n    @property\n    def processed_done_file(self):\n        return f\"processed_{self.norm_method}_{self.mask_dim}_{self.pe_dim}.done\"\n\n    @property\n    def processed_file_names(self):\n        return [self.processed_done_file]\n\n    def download(self):\n        pass\n\n    def process(self):\n        node_df = pd.read_csv(osp.join(self.raw_dir, \"pf_node.csv\"))\n        edge_df = pd.read_csv(osp.join(self.raw_dir, \"pf_edge.csv\"))\n\n        # Check the unique scenarios available\n        scenarios = node_df[\"scenario\"].unique()\n        # Ensure node and edge data match\n        if not (scenarios == edge_df[\"scenario\"].unique()).all():\n            raise ValueError(\"Mismatch between node and edge scenario values.\")\n\n        # normalize node attributes\n        cols_to_normalize = [\"Pd\", \"Qd\", \"Pg\", \"Qg\", \"Vm\", \"Va\"]\n        to_normalize = torch.tensor(\n            node_df[cols_to_normalize].values,\n            dtype=torch.float,\n        )\n        self.node_stats = self.node_normalizer.fit(to_normalize)\n        node_df[cols_to_normalize] = self.node_normalizer.transform(\n            to_normalize,\n        ).numpy()\n\n        # normalize edge attributes\n        cols_to_normalize = [\"G\", \"B\"]\n        to_normalize = torch.tensor(\n            edge_df[cols_to_normalize].values,\n            dtype=torch.float,\n        )\n        if isinstance(self.node_normalizer, BaseMVANormalizer):\n            self.edge_stats = self.edge_normalizer.fit(\n                to_normalize,\n                self.node_normalizer.baseMVA,\n            )\n        else:\n            self.edge_stats = self.edge_normalizer.fit(to_normalize)\n        edge_df[cols_to_normalize] = self.edge_normalizer.transform(\n            to_normalize,\n        ).numpy()\n\n        # save stats\n        node_stats_path = osp.join(\n            self.processed_dir,\n            f\"node_stats_{self.norm_method}.pt\",\n        )\n        edge_stats_path = osp.join(\n            self.processed_dir,\n            f\"edge_stats_{self.norm_method}.pt\",\n        )\n        torch.save(self.node_stats, node_stats_path)\n        torch.save(self.edge_stats, edge_stats_path)\n\n        # Create groupby objects for scenarios\n        node_groups = node_df.groupby(\"scenario\")\n        edge_groups = edge_df.groupby(\"scenario\")\n\n        for scenario_idx in tqdm(scenarios):\n            # NODE DATA\n            node_data = node_groups.get_group(scenario_idx)\n            x = torch.tensor(\n                node_data[\n                    [\"Pd\", \"Qd\", \"Pg\", \"Qg\", \"Vm\", \"Va\", \"PQ\", \"PV\", \"REF\"]\n                ].values,\n                dtype=torch.float,\n            )\n            y = x[:, : self.mask_dim]\n\n            # EDGE DATA\n            edge_data = edge_groups.get_group(scenario_idx)\n            edge_attr = torch.tensor(edge_data[[\"G\", \"B\"]].values, dtype=torch.float)\n            edge_index = torch.tensor(\n                edge_data[[\"index1\", \"index2\"]].values.T,\n                dtype=torch.long,\n            )\n\n            # Create the Data object\n            graph_data = Data(\n                x=x,\n                edge_index=edge_index,\n                edge_attr=edge_attr,\n                y=y,\n                scenario_id=scenario_idx,\n            )\n            pe_pre_transform = AddEdgeWeights()\n            graph_data = pe_pre_transform(graph_data)\n            pe_transform = AddNormalizedRandomWalkPE(\n                walk_length=self.pe_dim,\n                attr_name=\"pe\",\n            )\n            graph_data = pe_transform(graph_data)\n            torch.save(\n                graph_data,\n                osp.join(\n                    self.processed_dir,\n                    f\"data_{self.norm_method}_{self.mask_dim}_{self.pe_dim}_index_{scenario_idx}.pt\",\n                ),\n            )\n        with open(osp.join(self.processed_dir, self.processed_done_file), \"w\") as f:\n            f.write(\"done\")\n\n    def len(self):\n        if self.length is None:\n            files = [\n                f\n                for f in os.listdir(self.processed_dir)\n                if f.startswith(\n                    f\"data_{self.norm_method}_{self.mask_dim}_{self.pe_dim}_index_\",\n                )\n                and f.endswith(\".pt\")\n            ]\n            self.length = len(files)\n        return self.length\n\n    def get(self, idx):\n        file_name = osp.join(\n            self.processed_dir,\n            f\"data_{self.norm_method}_{self.mask_dim}_{self.pe_dim}_index_{idx}.pt\",\n        )\n        if not osp.exists(file_name):\n            raise IndexError(f\"Data file {file_name} does not exist.\")\n        data = torch.load(file_name, weights_only=False)\n        if self.transform:\n            data = self.transform(data)\n        return data\n\n    def change_transform(self, new_transform):\n        \"\"\"\n        Temporarily switch to a new transform function, used when evaluating different tasks.\n\n        Args:\n            new_transform (Callable): The new transform to use.\n        \"\"\"\n        self.original_transform = self.transform\n        self.transform = new_transform\n\n    def reset_transform(self):\n        \"\"\"\n        Reverts the transform to the original one set during initialization, usually called after the evaluation step.\n        \"\"\"\n        if self.original_transform is None:\n            raise ValueError(\n                \"The original transform is None or the function change_transform needs to be called before\",\n            )\n        self.transform = self.original_transform\n</code></pre> <code>change_transform(new_transform)</code> \u00b6 <p>Temporarily switch to a new transform function, used when evaluating different tasks.</p> <p>Parameters:</p> Name Type Description Default <code>new_transform</code> <code>Callable</code> <p>The new transform to use.</p> required Source code in <code>gridfm_graphkit/datasets/powergrid_dataset.py</code> <pre><code>def change_transform(self, new_transform):\n    \"\"\"\n    Temporarily switch to a new transform function, used when evaluating different tasks.\n\n    Args:\n        new_transform (Callable): The new transform to use.\n    \"\"\"\n    self.original_transform = self.transform\n    self.transform = new_transform\n</code></pre> <code>reset_transform()</code> \u00b6 <p>Reverts the transform to the original one set during initialization, usually called after the evaluation step.</p> Source code in <code>gridfm_graphkit/datasets/powergrid_dataset.py</code> <pre><code>def reset_transform(self):\n    \"\"\"\n    Reverts the transform to the original one set during initialization, usually called after the evaluation step.\n    \"\"\"\n    if self.original_transform is None:\n        raise ValueError(\n            \"The original transform is None or the function change_transform needs to be called before\",\n        )\n    self.transform = self.original_transform\n</code></pre>"},{"location":"datasets/transforms/","title":"Transforms","text":"<p>Each transformation class inherits from <code>BaseTransform</code> provided by PyTorch Geometric.</p>"},{"location":"datasets/transforms/#addnormalizedrandomwalkpe","title":"<code>AddNormalizedRandomWalkPE</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Adds the random walk positional encoding from the Graph Neural Networks with Learnable Structural and Positional Representations paper to the given graph. This is an adaptation from the original Pytorch Geometric implementation.</p> <p>Parameters:</p> Name Type Description Default <code>walk_length</code> <code>int</code> <p>The number of random walk steps.</p> required <code>attr_name</code> <code>str</code> <p>The attribute name of the data object to add positional encodings to. If set to :obj:<code>None</code>, will be concatenated to :obj:<code>data.x</code>. (default: :obj:<code>\"random_walk_pe\"</code>)</p> <code>'random_walk_pe'</code> Source code in <code>gridfm_graphkit/datasets/transforms.py</code> <pre><code>class AddNormalizedRandomWalkPE(BaseTransform):\n    r\"\"\"Adds the random walk positional encoding from the\n    [Graph Neural Networks with Learnable Structural and Positional Representations](https://arxiv.org/abs/2110.07875)\n    paper to the given graph. This is an adaptation from the original Pytorch Geometric implementation.\n\n    Args:\n        walk_length (int): The number of random walk steps.\n        attr_name (str, optional): The attribute name of the data object to add\n            positional encodings to. If set to :obj:`None`, will be\n            concatenated to :obj:`data.x`.\n            (default: :obj:`\"random_walk_pe\"`)\n    \"\"\"\n\n    def __init__(\n        self,\n        walk_length: int,\n        attr_name: Optional[str] = \"random_walk_pe\",\n    ) -&gt; None:\n        self.walk_length = walk_length\n        self.attr_name = attr_name\n\n    def forward(self, data: Data) -&gt; Data:\n        if data.edge_index is None:\n            raise ValueError(\"Expected data.edge_index to be not None\")\n        row, col = data.edge_index\n        N = data.num_nodes\n        if N is None:\n            raise ValueError(\"Expected data.num_nodes to be not None\")\n\n        if N &lt;= 2_000:  # Dense code path for faster computation:\n            adj = torch.zeros((N, N), device=row.device)\n            adj[row, col] = data.edge_weight\n            loop_index = torch.arange(N, device=row.device)\n        elif torch_geometric.typing.WITH_WINDOWS:\n            adj = to_torch_coo_tensor(\n                data.edge_index,\n                data.edge_weight,\n                size=data.size(),\n            )\n        else:\n            adj = to_torch_csr_tensor(\n                data.edge_index,\n                data.edge_weight,\n                size=data.size(),\n            )\n\n        row_sums = adj.sum(dim=1, keepdim=True)  # Sum along rows\n        row_sums = row_sums.clamp(min=1e-8)  # Prevent division by zero\n\n        adj = adj / row_sums  # Normalize each row to sum to 1\n\n        def get_pe(out: Tensor) -&gt; Tensor:\n            if is_torch_sparse_tensor(out):\n                return get_self_loop_attr(*to_edge_index(out), num_nodes=N)\n            return out[loop_index, loop_index]\n\n        out = adj\n        pe_list = [get_pe(out)]\n        for _ in range(self.walk_length - 1):\n            out = out @ adj\n            pe_list.append(get_pe(out))\n\n        pe = torch.stack(pe_list, dim=-1)\n        data[self.attr_name] = pe\n\n        return data\n</code></pre>"},{"location":"datasets/transforms/#addedgeweights","title":"<code>AddEdgeWeights</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Computes and adds edge weight as the magnitude of complex admittance.</p> <p>The magnitude is computed from the G and B components in <code>data.edge_attr</code> and stored in <code>data.edge_weight</code>.</p> Source code in <code>gridfm_graphkit/datasets/transforms.py</code> <pre><code>class AddEdgeWeights(BaseTransform):\n    \"\"\"\n    Computes and adds edge weight as the magnitude of complex admittance.\n\n    The magnitude is computed from the G and B components in `data.edge_attr` and stored in `data.edge_weight`.\n    \"\"\"\n\n    def forward(self, data):\n        if not hasattr(data, \"edge_attr\"):\n            raise AttributeError(\"Data must have 'edge_attr'.\")\n\n        # Extract real and imaginary parts of admittance\n        real = data.edge_attr[:, G]\n        imag = data.edge_attr[:, B]\n\n        # Compute the magnitude of the complex admittance\n        edge_weight = torch.sqrt(real**2 + imag**2)\n\n        # Add the computed edge weights to the data object\n        data.edge_weight = edge_weight\n\n        return data\n</code></pre>"},{"location":"datasets/transforms/#addidentitymask","title":"<code>AddIdentityMask</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Creates an identity mask, and adds it as a <code>mask</code> attribute.</p> <p>The mask is generated such that every entry is False, so no masking is actually applied</p> Source code in <code>gridfm_graphkit/datasets/transforms.py</code> <pre><code>@MASKING_REGISTRY.register(\"none\")\nclass AddIdentityMask(BaseTransform):\n    \"\"\"Creates an identity mask, and adds it as a `mask` attribute.\n\n    The mask is generated such that every entry is False, so no masking is actually applied\n    \"\"\"\n\n    def __init__(self, args):\n        super().__init__()\n\n    def forward(self, data):\n        if not hasattr(data, \"y\"):\n            raise AttributeError(\"Data must have ground truth 'y'.\")\n\n        # Generate an identity mask\n        mask = torch.zeros_like(data.y, dtype=torch.bool)\n\n        # Add the mask to the data object\n        data.mask = mask\n\n        return data\n</code></pre>"},{"location":"datasets/transforms/#addrandommask","title":"<code>AddRandomMask</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Creates a random mask, and adds it as a <code>mask</code> attribute.</p> <p>The mask is generated such that each entry is <code>True</code> with probability <code>mask_ratio</code> and <code>False</code> otherwise.</p> Source code in <code>gridfm_graphkit/datasets/transforms.py</code> <pre><code>@MASKING_REGISTRY.register(\"rnd\")\nclass AddRandomMask(BaseTransform):\n    \"\"\"Creates a random mask, and adds it as a `mask` attribute.\n\n    The mask is generated such that each entry is `True` with probability\n    `mask_ratio` and `False` otherwise.\n    \"\"\"\n\n    def __init__(self, args):\n        super().__init__()\n        self.mask_dim = args.data.mask_dim\n        self.mask_ratio = args.data.mask_ratio\n\n    def forward(self, data):\n        if not hasattr(data, \"x\"):\n            raise AttributeError(\"Data must have node features 'x'.\")\n\n        # Generate a random mask\n        mask = torch.rand(data.x.size(0), self.mask_dim) &lt; self.mask_ratio\n\n        # Add the mask to the data object\n        data.mask = mask\n\n        return data\n</code></pre>"},{"location":"datasets/transforms/#addpfmask","title":"<code>AddPFMask</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Creates a mask according to the power flow problem and assigns it as a <code>mask</code> attribute.</p> Source code in <code>gridfm_graphkit/datasets/transforms.py</code> <pre><code>@MASKING_REGISTRY.register(\"pf\")\nclass AddPFMask(BaseTransform):\n    \"\"\"Creates a mask according to the power flow problem and assigns it as a `mask` attribute.\"\"\"\n\n    def __init__(self, args):\n        super().__init__()\n\n    def forward(self, data):\n        # Ensure the data object has the required attributes\n        if not hasattr(data, \"y\"):\n            raise AttributeError(\"Data must have ground truth 'y'.\")\n\n        if not hasattr(data, \"x\"):\n            raise AttributeError(\"Data must have node features 'x'.\")\n\n        # Generate masks for each type of node\n        mask_PQ = data.x[:, PQ] == 1  # PQ buses\n        mask_PV = data.x[:, PV] == 1  # PV buses\n        mask_REF = data.x[:, REF] == 1  # Reference buses\n\n        # Initialize the mask tensor with False values\n        mask = torch.zeros_like(data.y, dtype=torch.bool)\n\n        mask[mask_PQ, VM] = True  # Mask Vm for PQ buses\n        mask[mask_PQ, VA] = True  # Mask Va for PQ buses\n\n        mask[mask_PV, QG] = True  # Mask Qg for PV buses\n        mask[mask_PV, VA] = True  # Mask Va for PV buses\n\n        mask[mask_REF, PG] = True  # Mask Pg for REF buses\n        mask[mask_REF, QG] = True  # Mask Qg for REF buses\n\n        # Attach the mask to the data object\n        data.mask = mask\n\n        return data\n</code></pre>"},{"location":"datasets/transforms/#addopfmask","title":"<code>AddOPFMask</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Creates a mask according to the optimal power flow problem and assigns it as a <code>mask</code> attribute.</p> Source code in <code>gridfm_graphkit/datasets/transforms.py</code> <pre><code>@MASKING_REGISTRY.register(\"opf\")\nclass AddOPFMask(BaseTransform):\n    \"\"\"Creates a mask according to the optimal power flow problem and assigns it as a `mask` attribute.\"\"\"\n\n    def __init__(self, args):\n        super().__init__()\n\n    def forward(self, data):\n        # Ensure the data object has the required attributes\n        if not hasattr(data, \"y\"):\n            raise AttributeError(\"Data must have ground truth 'y'.\")\n\n        if not hasattr(data, \"x\"):\n            raise AttributeError(\"Data must have node features 'x'.\")\n\n        # Generate masks for each type of node\n        mask_PQ = data.x[:, PQ] == 1  # PQ buses\n        mask_PV = data.x[:, PV] == 1  # PV buses\n        mask_REF = data.x[:, REF] == 1  # Reference buses\n\n        # Initialize the mask tensor with False values\n        mask = torch.zeros_like(data.y, dtype=torch.bool)\n\n        mask[mask_PQ, VM] = True  # Mask Vm for PQ\n        mask[mask_PQ, VA] = True  # Mask Va for PQ\n\n        mask[mask_PV, PG] = True  # Mask Pg for PV\n        mask[mask_PV, QG] = True  # Mask Qg for PV\n        mask[mask_PV, VM] = True  # Mask Vm for PV\n        mask[mask_PV, VA] = True  # Mask Va for PV\n\n        mask[mask_REF, PG] = True  # Mask Pg for REF\n        mask[mask_REF, QG] = True  # Mask Qg for REF\n        mask[mask_REF, VM] = True  # Mask Vm for REF\n        mask[mask_REF, VA] = True  # Mask Va for REF\n\n        # Attach the mask to the data object\n        data.mask = mask\n\n        return data\n</code></pre>"},{"location":"install/installation/","title":"Installation","text":"<p>You can install <code>gridfm-graphkit</code> directly from PyPI:</p> <pre><code>pip install gridfm-graphkit\n</code></pre>"},{"location":"install/installation/#development-setup","title":"Development Setup","text":"<p>To contribute or develop locally, clone the repository and install in editable mode:</p> <pre><code>git clone git@github.com:gridfm/gridfm-graphkit.git\ncd gridfm-graphkit\npython -m venv venv\nsource venv/bin/activate\npip install -e .\n</code></pre> <p>For documentation generation and unit testing, install with the optional <code>dev</code> and <code>test</code> extras:</p> <pre><code>pip install -e .[dev,test]\n</code></pre>"},{"location":"models/models/","title":"Models","text":""},{"location":"models/models/#gpstransformer","title":"<code>GPSTransformer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A GPS (Graph Transformer) model based on GPSConv and GINEConv layers from Pytorch Geometric.</p> <p>This model encodes node features and positional encodings separately, then applies multiple graph convolution layers with batch normalization, and finally decodes to the output dimension.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>NestedNamespace</code> <p>Parameters</p> required <p>Attributes:</p> Name Type Description <code>input_dim</code> <code>int</code> <p>Dimension of input node features. From <code>args.model.input_dim</code>.</p> <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size for all layers. From <code>args.model.hidden_size</code>.</p> <code>output_dim</code> <code>int</code> <p>Dimension of the output node features. From <code>args.model.output_dim</code>.</p> <code>edge_dim</code> <code>int</code> <p>Dimension of edge features. From <code>args.model.edge_dim</code>.</p> <code>pe_dim</code> <code>int</code> <p>Dimension of the positional encoding. Must be less than <code>hidden_dim</code>. From <code>args.model.pe_dim</code>.</p> <code>num_layers</code> <code>int</code> <p>Number of GPSConv layers. From <code>args.model.num_layers</code>.</p> <code>heads</code> <code>int</code> <p>Number of attention heads in GPSConv. From <code>args.model.attention_head</code>. Defaults to 1.</p> <code>dropout</code> <code>float</code> <p>Dropout rate in GPSConv. From <code>args.model.dropout</code>. Defaults to 0.0.</p> <code>mask_dim</code> <code>int</code> <p>Dimension of the mask vector. From <code>args.data.mask_dim</code>. Defaults to 6.</p> <code>mask_value</code> <code>float</code> <p>Initial value for learnable mask parameters. From <code>args.data.mask_value</code>. Defaults to -1.0.</p> <code>learn_mask</code> <code>bool</code> <p>Whether to learn mask values as parameters. From <code>args.data.learn_mask</code>. Defaults to True.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>pe_dim</code> is not less than <code>hidden_dim</code>.</p> Source code in <code>gridfm_graphkit/models/gps_transformer.py</code> <pre><code>@MODELS_REGISTRY.register(\"GPSTransformer\")\nclass GPSTransformer(nn.Module):\n    \"\"\"\n    A GPS (Graph Transformer) model based on [GPSConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GPSConv.html) and [GINEConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GINEConv.html) layers from Pytorch Geometric.\n\n    This model encodes node features and positional encodings separately,\n    then applies multiple graph convolution layers with batch normalization,\n    and finally decodes to the output dimension.\n\n    Args:\n        args (NestedNamespace): Parameters\n\n    Attributes:\n        input_dim (int): Dimension of input node features. From ``args.model.input_dim``.\n        hidden_dim (int): Hidden dimension size for all layers. From ``args.model.hidden_size``.\n        output_dim (int): Dimension of the output node features. From ``args.model.output_dim``.\n        edge_dim (int): Dimension of edge features. From ``args.model.edge_dim``.\n        pe_dim (int): Dimension of the positional encoding. Must be less than ``hidden_dim``. From ``args.model.pe_dim``.\n        num_layers (int): Number of GPSConv layers. From ``args.model.num_layers``.\n        heads (int, optional): Number of attention heads in GPSConv. From ``args.model.attention_head``. Defaults to 1.\n        dropout (float, optional): Dropout rate in GPSConv. From ``args.model.dropout``. Defaults to 0.0.\n        mask_dim (int, optional): Dimension of the mask vector. From ``args.data.mask_dim``. Defaults to 6.\n        mask_value (float, optional): Initial value for learnable mask parameters. From ``args.data.mask_value``. Defaults to -1.0.\n        learn_mask (bool, optional): Whether to learn mask values as parameters. From ``args.data.learn_mask``. Defaults to True.\n\n    Raises:\n        ValueError: If `pe_dim` is not less than `hidden_dim`.\n    \"\"\"\n\n    def __init__(self, args):\n        super().__init__()\n\n        # === Required (no defaults in original) ===\n        self.input_dim = args.model.input_dim\n        self.hidden_dim = args.model.hidden_size\n        self.output_dim = args.model.output_dim\n        self.edge_dim = args.model.edge_dim\n        self.pe_dim = args.model.pe_dim\n        self.num_layers = args.model.num_layers\n\n        # === Optional (defaults in original) ===\n        self.heads = getattr(args.model, \"attention_head\", 1)\n        self.dropout = getattr(args.model, \"dropout\", 0.0)\n        self.mask_dim = getattr(args.data, \"mask_dim\", 6)\n        self.mask_value = getattr(args.data, \"mask_value\", -1.0)\n        self.learn_mask = getattr(args.data, \"learn_mask\", True)\n\n        if not self.pe_dim &lt; self.hidden_dim:\n            raise ValueError(\n                \"positional encoding dimension must be smaller than model hidden dimension\",\n            )\n\n        self.layers = nn.ModuleList()\n\n        self.encoder = nn.Sequential(\n            nn.Linear(self.input_dim, self.hidden_dim - self.pe_dim),\n            nn.LeakyReLU(),\n        )\n        self.input_norm = nn.BatchNorm1d(self.hidden_dim - self.pe_dim)\n        self.pe_norm = nn.BatchNorm1d(self.pe_dim)\n\n        for _ in range(self.num_layers):\n            mlp = nn.Sequential(\n                nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim),\n                nn.LeakyReLU(),\n            )\n            self.layers.append(\n                nn.ModuleDict(\n                    {\n                        \"conv\": GPSConv(\n                            channels=self.hidden_dim,\n                            conv=GINEConv(nn=mlp, edge_dim=self.edge_dim),\n                            heads=self.heads,\n                            dropout=self.dropout,\n                        ),\n                        \"norm\": nn.BatchNorm1d(\n                            self.hidden_dim,\n                        ),  # BatchNorm after each graph layer\n                    },\n                ),\n            )\n\n        self.pre_decoder_norm = nn.BatchNorm1d(self.hidden_dim)\n        # Fully connected (MLP) layers after the GAT layers\n        self.decoder = nn.Sequential(\n            nn.Linear(self.hidden_dim, self.hidden_dim),\n            nn.LeakyReLU(),\n            nn.Linear(self.hidden_dim, self.output_dim),\n        )\n\n        if self.learn_mask:\n            self.mask_value = nn.Parameter(\n                torch.randn(self.mask_dim) + self.mask_value,\n                requires_grad=True,\n            )\n        else:\n            self.mask_value = nn.Parameter(\n                torch.zeros(self.mask_dim) + self.mask_value,\n                requires_grad=False,\n            )\n\n    def forward(self, x, pe, edge_index, edge_attr, batch):\n        \"\"\"\n        Forward pass for the GPSTransformer.\n\n        Args:\n            x (Tensor): Input node features of shape [num_nodes, input_dim].\n            pe (Tensor): Positional encoding of shape [num_nodes, pe_dim].\n            edge_index (Tensor): Edge indices for graph convolution.\n            edge_attr (Tensor): Edge feature tensor.\n            batch (Tensor): Batch vector assigning nodes to graphs.\n\n        Returns:\n            output (Tensor): Output node features of shape [num_nodes, output_dim].\n        \"\"\"\n        x_pe = self.pe_norm(pe)\n\n        x = self.encoder(x)\n        x = self.input_norm(x)\n\n        x = torch.cat((x, x_pe), 1)\n        for layer in self.layers:\n            x = layer[\"conv\"](\n                x=x,\n                edge_index=edge_index,\n                edge_attr=edge_attr,\n                batch=batch,\n            )\n            x = layer[\"norm\"](x)\n\n        x = self.pre_decoder_norm(x)\n        x = self.decoder(x)\n\n        return x\n</code></pre> <code>forward(x, pe, edge_index, edge_attr, batch)</code> \u00b6 <p>Forward pass for the GPSTransformer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input node features of shape [num_nodes, input_dim].</p> required <code>pe</code> <code>Tensor</code> <p>Positional encoding of shape [num_nodes, pe_dim].</p> required <code>edge_index</code> <code>Tensor</code> <p>Edge indices for graph convolution.</p> required <code>edge_attr</code> <code>Tensor</code> <p>Edge feature tensor.</p> required <code>batch</code> <code>Tensor</code> <p>Batch vector assigning nodes to graphs.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Output node features of shape [num_nodes, output_dim].</p> Source code in <code>gridfm_graphkit/models/gps_transformer.py</code> <pre><code>def forward(self, x, pe, edge_index, edge_attr, batch):\n    \"\"\"\n    Forward pass for the GPSTransformer.\n\n    Args:\n        x (Tensor): Input node features of shape [num_nodes, input_dim].\n        pe (Tensor): Positional encoding of shape [num_nodes, pe_dim].\n        edge_index (Tensor): Edge indices for graph convolution.\n        edge_attr (Tensor): Edge feature tensor.\n        batch (Tensor): Batch vector assigning nodes to graphs.\n\n    Returns:\n        output (Tensor): Output node features of shape [num_nodes, output_dim].\n    \"\"\"\n    x_pe = self.pe_norm(pe)\n\n    x = self.encoder(x)\n    x = self.input_norm(x)\n\n    x = torch.cat((x, x_pe), 1)\n    for layer in self.layers:\n        x = layer[\"conv\"](\n            x=x,\n            edge_index=edge_index,\n            edge_attr=edge_attr,\n            batch=batch,\n        )\n        x = layer[\"norm\"](x)\n\n    x = self.pre_decoder_norm(x)\n    x = self.decoder(x)\n\n    return x\n</code></pre>"},{"location":"models/models/#gnn_transformerconv","title":"<code>GNN_TransformerConv</code>","text":"<p>               Bases: <code>Module</code></p> <p>Graph Neural Network using TransformerConv layers from PyTorch Geometric.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>NestedNamespace</code> <p>Parameters</p> required <p>Attributes:</p> Name Type Description <code>input_dim</code> <code>int</code> <p>Dimensionality of input node features. From <code>args.model.input_dim</code>.</p> <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size for TransformerConv layers. From <code>args.model.hidden_dim</code>.</p> <code>output_dim</code> <code>int</code> <p>Output dimension size. From <code>args.model.output_dim</code>.</p> <code>edge_dim</code> <code>int</code> <p>Dimensionality of edge features. From <code>args.model.edge_dim</code>.</p> <code>num_layers</code> <code>int</code> <p>Number of TransformerConv layers. From <code>args.model.num_layers</code>.</p> <code>heads</code> <code>int</code> <p>Number of attention heads. From <code>args.model.heads</code>. Defaults to 1.</p> <code>mask_dim</code> <code>int</code> <p>Dimension of mask vector. From <code>args.data.mask_dim</code>. Defaults to 6.</p> <code>mask_value</code> <code>float</code> <p>Initial mask value. From <code>args.data.mask_value</code>. Defaults to -1.0.</p> <code>learn_mask</code> <code>bool</code> <p>Whether mask values are learnable. From <code>args.data.learn_mask</code>. Defaults to True.</p> Source code in <code>gridfm_graphkit/models/gnn_transformer.py</code> <pre><code>@MODELS_REGISTRY.register(\"GNN_TransformerConv\")\nclass GNN_TransformerConv(nn.Module):\n    \"\"\"\n    Graph Neural Network using [TransformerConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.TransformerConv.html) layers from PyTorch Geometric.\n\n    Args:\n        args (NestedNamespace): Parameters\n\n    Attributes:\n        input_dim (int): Dimensionality of input node features. From ``args.model.input_dim``.\n        hidden_dim (int): Hidden dimension size for TransformerConv layers. From ``args.model.hidden_dim``.\n        output_dim (int): Output dimension size. From ``args.model.output_dim``.\n        edge_dim (int): Dimensionality of edge features. From ``args.model.edge_dim``.\n        num_layers (int): Number of TransformerConv layers. From ``args.model.num_layers``.\n        heads (int, optional): Number of attention heads. From ``args.model.heads``. Defaults to 1.\n        mask_dim (int, optional): Dimension of mask vector. From ``args.data.mask_dim``. Defaults to 6.\n        mask_value (float, optional): Initial mask value. From ``args.data.mask_value``. Defaults to -1.0.\n        learn_mask (bool, optional): Whether mask values are learnable. From ``args.data.learn_mask``. Defaults to True.\n    \"\"\"\n\n    def __init__(self, args):\n        super().__init__()\n\n        # === Required (no defaults originally) ===\n        self.input_dim = args.model.input_dim\n        self.hidden_dim = args.model.hidden_size\n        self.output_dim = args.model.output_dim\n        self.edge_dim = args.model.edge_dim\n        self.num_layers = args.model.num_layers\n\n        # === Optional (had defaults originally) ===\n        self.heads = getattr(args.model, \"attention_head\", 1)\n        self.mask_dim = getattr(args.data, \"mask_dim\", 6)\n        self.mask_value = getattr(args.data, \"mask_value\", -1.0)\n        self.learn_mask = getattr(args.data, \"learn_mask\", False)\n\n        self.layers = nn.ModuleList()\n        current_dim = self.input_dim  # First layer takes `input_dim` as input\n\n        for _ in range(self.num_layers):\n            self.layers.append(\n                TransformerConv(\n                    current_dim,\n                    self.hidden_dim,\n                    heads=self.heads,\n                    edge_dim=self.edge_dim,\n                    beta=False,\n                ),\n            )\n            # Update the dimension for the next layer\n            current_dim = self.hidden_dim * self.heads\n\n        # Fully connected (MLP) layers after the GAT layers\n        self.mlps = nn.Sequential(\n            nn.Linear(self.hidden_dim * self.heads, self.hidden_dim),\n            nn.LeakyReLU(),\n            nn.Linear(self.hidden_dim, self.output_dim),\n        )\n\n        if self.learn_mask:\n            self.mask_value = nn.Parameter(\n                torch.randn(self.mask_dim) + self.mask_value,\n                requires_grad=True,\n            )\n        else:\n            self.mask_value = nn.Parameter(\n                torch.zeros(self.mask_dim) + self.mask_value,\n                requires_grad=False,\n            )\n\n    def forward(self, x, pe, edge_index, edge_attr, batch):\n        \"\"\"\n        Forward pass for the GPSTransformer.\n\n        Args:\n            x (Tensor): Input node features of shape [num_nodes, input_dim].\n            pe (Tensor): Positional encoding of shape [num_nodes, pe_dim] (not used).\n            edge_index (Tensor): Edge indices for graph convolution.\n            edge_attr (Tensor): Edge feature tensor.\n            batch (Tensor): Batch vector assigning nodes to graphs (not used).\n\n        Returns:\n            output (Tensor): Output node features of shape [num_nodes, output_dim].\n        \"\"\"\n        for conv in self.layers:\n            x = conv(x, edge_index, edge_attr)\n            x = nn.LeakyReLU()(x)\n\n        x = self.mlps(x)\n        return x\n</code></pre> <code>forward(x, pe, edge_index, edge_attr, batch)</code> \u00b6 <p>Forward pass for the GPSTransformer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input node features of shape [num_nodes, input_dim].</p> required <code>pe</code> <code>Tensor</code> <p>Positional encoding of shape [num_nodes, pe_dim] (not used).</p> required <code>edge_index</code> <code>Tensor</code> <p>Edge indices for graph convolution.</p> required <code>edge_attr</code> <code>Tensor</code> <p>Edge feature tensor.</p> required <code>batch</code> <code>Tensor</code> <p>Batch vector assigning nodes to graphs (not used).</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Output node features of shape [num_nodes, output_dim].</p> Source code in <code>gridfm_graphkit/models/gnn_transformer.py</code> <pre><code>def forward(self, x, pe, edge_index, edge_attr, batch):\n    \"\"\"\n    Forward pass for the GPSTransformer.\n\n    Args:\n        x (Tensor): Input node features of shape [num_nodes, input_dim].\n        pe (Tensor): Positional encoding of shape [num_nodes, pe_dim] (not used).\n        edge_index (Tensor): Edge indices for graph convolution.\n        edge_attr (Tensor): Edge feature tensor.\n        batch (Tensor): Batch vector assigning nodes to graphs (not used).\n\n    Returns:\n        output (Tensor): Output node features of shape [num_nodes, output_dim].\n    \"\"\"\n    for conv in self.layers:\n        x = conv(x, edge_index, edge_attr)\n        x = nn.LeakyReLU()(x)\n\n    x = self.mlps(x)\n    return x\n</code></pre>"},{"location":"quick_start/quick_start/","title":"CLI commands","text":"<p>An interface to train, fine-tune, and evaluate GridFM models using configurable YAML files and MLflow tracking.</p> <pre><code>gridfm_graphkit &lt;command&gt; [OPTIONS]\n</code></pre> <p>Available commands:</p> <ul> <li><code>train</code> \u2013 Train a new model from scratch</li> <li><code>finetune</code> \u2013 Fine-tune an existing pre-trained model</li> <li><code>evaluate</code> \u2013 Evaluate model performance on a dataset</li> <li><code>predict</code> \u2013 Run inference and save predictions</li> </ul>"},{"location":"quick_start/quick_start/#training-models","title":"Training Models","text":"<pre><code>gridfm_graphkit train --config path/to/config.yaml\n</code></pre>"},{"location":"quick_start/quick_start/#arguments","title":"Arguments","text":"Argument Type Description Default <code>--config</code> <code>str</code> Required. Path to the training configuration YAML file. <code>None</code> <code>--exp_name</code> <code>str</code> Optional. MLflow experiment name. <code>timestamp</code> <code>--run_name</code> <code>str</code> Optional. MLflow run name. <code>run</code> <code>--log_dir</code> <code>str</code> Optional. MLflow logging directory. <code>mlruns</code> <code>--data_path</code> <code>str</code> Optional. Root dataset directory. <code>data</code>"},{"location":"quick_start/quick_start/#examples","title":"Examples","text":"<p>Standard Training:</p> <pre><code>gridfm_graphkit train --config examples/config/case30_ieee_base.yaml --data_path examples/data\n</code></pre>"},{"location":"quick_start/quick_start/#fine-tuning-models","title":"Fine-Tuning Models","text":"<pre><code>gridfm_graphkit finetune --config path/to/config.yaml --model_path path/to/model.pth\n</code></pre>"},{"location":"quick_start/quick_start/#arguments_1","title":"Arguments","text":"Argument Type Description Default <code>--config</code> <code>str</code> Required. Fine-tuning configuration file. <code>None</code> <code>--model_path</code> <code>str</code> Required. Path to a pre-trained model file. <code>None</code> <code>--exp_name</code> <code>str</code> MLflow experiment name. timestamp <code>--run_name</code> <code>str</code> MLflow run name. <code>run</code> <code>--log_dir</code> <code>str</code> MLflow logging directory. <code>mlruns</code> <code>--data_path</code> <code>str</code> Root dataset directory. <code>data</code>"},{"location":"quick_start/quick_start/#evaluating-models","title":"Evaluating Models","text":"<pre><code>gridfm_graphkit evaluate --config path/to/eval.yaml --model_path path/to/model.pth\n</code></pre>"},{"location":"quick_start/quick_start/#arguments_2","title":"Arguments","text":"Argument Type Description Default <code>--config</code> <code>str</code> Required. Path to evaluation config. <code>None</code> <code>--model_path</code> <code>str</code> Path to the trained model file. <code>None</code> <code>--exp_name</code> <code>str</code> MLflow experiment name. timestamp <code>--run_name</code> <code>str</code> MLflow run name. <code>run</code> <code>--log_dir</code> <code>str</code> MLflow logging directory. <code>mlruns</code> <code>--data_path</code> <code>str</code> Dataset directory. <code>data</code>"},{"location":"quick_start/quick_start/#running-predictions","title":"Running Predictions","text":"<pre><code>gridfm_graphkit predict --config path/to/config.yaml --model_path path/to/model.pth\n</code></pre>"},{"location":"quick_start/quick_start/#arguments_3","title":"Arguments","text":"Argument Type Description Default <code>--config</code> <code>str</code> Required. Path to prediction config file. <code>None</code> <code>--model_path</code> <code>str</code> Path to the trained model file. <code>None</code> <code>--exp_name</code> <code>str</code> MLflow experiment name. timestamp <code>--run_name</code> <code>str</code> MLflow run name. <code>run</code> <code>--log_dir</code> <code>str</code> MLflow logging directory. <code>mlruns</code> <code>--data_path</code> <code>str</code> Dataset directory. <code>data</code> <code>--output_path</code> <code>str</code> Directory where predictions are saved. <code>data</code>"},{"location":"quick_start/yaml_config/","title":"The YAML configuration file","text":"<p>Every experiment in <code>gridfm-graphkit</code> is defined through a single YAML configuration file. This file specifies which networks to load, how to normalize the data, which model architecture to build, and how different stages of the workflow should be executed.</p> <p>Rather than modifying the source code, you simply adjust the YAML file to describe your experiment. This approach makes results reproducible and easy to share: all the important details are stored in one place.</p> <p>The configuration is divided into sections (<code>data</code>, <code>model</code>, <code>training</code>, <code>optimizer</code>, etc.), with each section grouping related options. We will explain these fields one by one and show how to use them effectively.</p> <p>For ready-to-use examples, check the folder <code>examples/config/</code>, which contains valid configuration files you can adapt for your own experiments.</p>"},{"location":"quick_start/yaml_config/#data","title":"Data","text":"<p>The <code>data</code> section defines which networks and scenarios to use, as well as how to prepare and mask the input features.</p> <p>Example:</p> <pre><code>data:\n  networks: [\"case300_ieee\", \"case30_ieee\"]\n  scenarios: [8500, 4000]\n  normalization: baseMVAnorm\n  baseMVA: 100\n  mask_type: rnd\n  mask_value: 0.0\n  mask_ratio: 0.5\n  mask_dim: 6\n  learn_mask: false\n  val_ratio: 0.1\n  test_ratio: 0.1\n  workers: 4\n</code></pre> <p>Key fields:</p> <ul> <li><code>networks</code>: List of network topologies (e.g., IEEE test cases) used.</li> <li><code>scenarios</code>: Number of scenarios (samples) for each network.</li> <li><code>normalization</code>: Method to scale features. Options:<ul> <li><code>minmax</code>: scale between min and max.</li> <li><code>standard</code>: zero mean, unit variance.</li> <li><code>baseMVAnorm</code>: divide by base MVA value (see <code>baseMVA</code>).</li> <li><code>identity</code>: no normalization.</li> </ul> </li> <li><code>baseMVA</code>: Base MVA value from the case file (default: 100).</li> <li><code>mask_type</code>: Defines how input features are masked:<ul> <li><code>rnd</code> = random masking (controlled by <code>mask_ratio</code> and <code>mask_dim</code>).</li> <li><code>pf</code> = power flow problem setup.</li> <li><code>opf</code> = optimal power flow setup.</li> <li><code>none</code> = no masking.</li> </ul> </li> <li><code>mask_value</code>: Numerical value used to mask inputs (default: 0.0).</li> <li><code>mask_ratio</code>: Probability of masking a feature (only used when <code>mask_type=rnd</code>).</li> <li><code>mask_dim</code>: Number of features that can masked (default: the first 6 \u2192 Pd, Qd, Pg, Qg, Vm, Va).</li> <li><code>learn_mask</code>: If true, the mask value becomes learnable.</li> <li><code>val_ratio</code> / <code>test_ratio</code>: Fractions of the dataset used for validation and testing.</li> <li><code>workers</code>: Number of data-loading workers</li> </ul>"},{"location":"quick_start/yaml_config/#model","title":"Model","text":"<p>The <code>model</code> section specifies the neural network architecture and its hyperparameters.</p> <p>Example:</p> <pre><code>model:\n  type: GPSconv\n  input_dim: 9\n  output_dim: 6\n  edge_dim: 2\n  pe_dim: 20\n  num_layers: 6\n  hidden_size: 256\n  attention_head: 8\n  dropout: 0.0\n</code></pre> <p>Key fields:</p> <ul> <li><code>type</code>: Model architecture (e.g., <code>\"GPSconv\"</code>).</li> <li><code>input_dim</code>: Input feature dimension (default: 9 \u2192 Pd, Qd, Pg, Qg, Vm, Va, PQ, PV, REF).</li> <li><code>output_dim</code>: Output feature dimension (default: 6 \u2192 Pd, Qd, Pg, Qg, Vm, Va).</li> <li><code>edge_dim</code>: Dimension of edge features (default: 2 \u2192 G, B).</li> <li><code>pe_dim</code>: Size of positional encoding (e.g., random walk length).</li> <li><code>num_layers</code>: Number of layers in the network.</li> <li><code>hidden_size</code>: Width of hidden layers.</li> <li><code>attention_head</code>: Number of attention heads.</li> <li><code>dropout</code>: Dropout probability (default: 0.0).</li> </ul>"},{"location":"quick_start/yaml_config/#training","title":"Training","text":"<p>The <code>training</code> section defines how the model is optimized and which loss functions are used.</p> <p>Example:</p> <pre><code>training:\n  batch_size: 16\n  epochs: 100\n  losses: [\"MaskedMSE\", \"PBE\"]\n  loss_weights: [0.01, 0.99]\n  accelerator: auto\n  devices: auto\n  strategy: auto\n</code></pre> <p>Key fields:</p> <ul> <li><code>batch_size</code>: Number of samples per training batch.</li> <li><code>epochs</code>: Number of training epochs.</li> <li><code>losses</code>: List of losses to combine. Options:<ul> <li><code>MSE</code> = Mean Squared Error.</li> <li><code>MaskedMSE</code> = Masked Mean Squared Error.</li> <li><code>SCE</code> = Scaled Cosine Error.</li> <li><code>PBE</code> = Power Balance Equation loss.</li> </ul> </li> <li><code>loss_weights</code>: Relative weights applied to each loss term.</li> <li><code>accelerator</code>: Device type used for training (cpu, gpu, mps, or auto).</li> <li><code>devices</code>: Number of devices (GPUs/CPUs) to use (or auto)</li> <li><code>strategy</code>: Training strategy (e.g., ddp for distributed data parallel, or auto).</li> </ul> <p>Note</p> <p>On macOS, using accelerator: <code>cpu</code> is often the most stable choice.</p>"},{"location":"quick_start/yaml_config/#optimizer","title":"Optimizer","text":"<p>Defines the optimizer and learning rate scheduling.</p> <p>Example:</p> <pre><code>optimizer:\n  learning_rate: 0.0001\n  beta1: 0.9\n  beta2: 0.999\n  lr_decay: 0.5\n  lr_patience: 3\n</code></pre> <p>Key fields:</p> <ul> <li><code>learning_rate</code>: Initial learning rate.</li> <li><code>beta1</code>, <code>beta2</code>: Adam optimizer parameters (defaults: 0.9, 0.999).</li> <li><code>lr_decay</code>: Factor to decay the learning rate.</li> <li><code>lr_patience</code>: Number of epochs to wait before reducing the LR.</li> </ul>"},{"location":"quick_start/yaml_config/#callbacks","title":"Callbacks","text":"<p>Callbacks add additional behavior during training, such as early stopping.</p> <p>Example:</p> <pre><code>callbacks:\n  patience: 100\n  tol: 0\n</code></pre> <p>Key fields:</p> <ul> <li><code>patience</code>: Number of epochs to wait before early stopping.</li> <li><code>tol</code>: Minimum improvement required in validation loss to reset patience.</li> </ul>"},{"location":"tasks/feature_reconstruction/","title":"Feature Reconstruction Task","text":"<p>               Bases: <code>LightningModule</code></p> <p>PyTorch Lightning task for node feature reconstruction on power grid graphs.</p> <p>This task wraps a GridFM model inside a LightningModule and defines the full training, validation, testing, and prediction logic. It is designed to reconstruct masked node features from graph-structured input data, using datasets and normalizers provided by <code>gridfm-graphkit</code>.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>NestedNamespace</code> <p>Experiment configuration. Expected fields include <code>training.batch_size</code>, <code>optimizer.*</code>, etc.</p> required <code>node_normalizers</code> <code>list</code> <p>One normalizer per dataset to (de)normalize node features.</p> required <code>edge_normalizers</code> <code>list</code> <p>One normalizer per dataset to (de)normalize edge features.</p> required <p>Attributes:</p> Name Type Description <code>model</code> <code>Module</code> <p>model loaded via <code>load_model</code>.</p> <code>loss_fn</code> <code>callable</code> <p>Loss function resolved from configuration.</p> <code>batch_size</code> <code>int</code> <p>Training batch size. From <code>args.training.batch_size</code></p> <code>node_normalizers</code> <code>list</code> <p>Dataset-wise node feature normalizers.</p> <code>edge_normalizers</code> <code>list</code> <p>Dataset-wise edge feature normalizers.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass with optional feature masking.</p> <code>training_step</code> <p>One training step: computes loss, logs metrics, returns loss.</p> <code>validation_step</code> <p>One validation step: computes losses and logs metrics.</p> <code>test_step</code> <p>Evaluate on test data, compute per-node-type MSEs, and log per-dataset metrics.</p> <code>predict_step</code> <p>Run inference and return denormalized outputs + node masks.</p> <code>configure_optimizers</code> <p>Setup Adam optimizer and ReduceLROnPlateau scheduler.</p> <code>on_fit_start</code> <p>Save normalization statistics at the beginning of training.</p> <code>on_test_end</code> <p>Collect test metrics across datasets and export summary CSV reports.</p> Notes <ul> <li>Node types are distinguished using the global constants (<code>PQ</code>, <code>PV</code>, <code>REF</code>).</li> <li>The datamodule must provide <code>batch.mask</code> for masking node features.</li> <li>Test metrics include per-node-type RMSE for [Pd, Qd, Pg, Qg, Vm, Va].</li> <li>Reports are saved under <code>&lt;mlflow_artifacts&gt;/test/&lt;dataset&gt;.csv</code>.</li> </ul> Example <pre><code>model = FeatureReconstructionTask(args, node_normalizers, edge_normalizers)\noutput = model(batch.x, batch.pe, batch.edge_index, batch.edge_attr, batch.batch)\n</code></pre> Source code in <code>gridfm_graphkit/tasks/feature_reconstruction_task.py</code> <pre><code>class FeatureReconstructionTask(L.LightningModule):\n    \"\"\"\n    PyTorch Lightning task for node feature reconstruction on power grid graphs.\n\n    This task wraps a GridFM model inside a LightningModule and defines the full\n    training, validation, testing, and prediction logic. It is designed to\n    reconstruct masked node features from graph-structured input data, using\n    datasets and normalizers provided by `gridfm-graphkit`.\n\n    Args:\n        args (NestedNamespace): Experiment configuration. Expected fields include `training.batch_size`, `optimizer.*`, etc.\n        node_normalizers (list): One normalizer per dataset to (de)normalize node features.\n        edge_normalizers (list): One normalizer per dataset to (de)normalize edge features.\n\n    Attributes:\n        model (torch.nn.Module): model loaded via `load_model`.\n        loss_fn (callable): Loss function resolved from configuration.\n        batch_size (int): Training batch size. From ``args.training.batch_size``\n        node_normalizers (list): Dataset-wise node feature normalizers.\n        edge_normalizers (list): Dataset-wise edge feature normalizers.\n\n    Methods:\n        forward(x, pe, edge_index, edge_attr, batch, mask=None):\n            Forward pass with optional feature masking.\n        training_step(batch):\n            One training step: computes loss, logs metrics, returns loss.\n        validation_step(batch, batch_idx):\n            One validation step: computes losses and logs metrics.\n        test_step(batch, batch_idx, dataloader_idx=0):\n            Evaluate on test data, compute per-node-type MSEs, and log per-dataset metrics.\n        predict_step(batch, batch_idx, dataloader_idx=0):\n            Run inference and return denormalized outputs + node masks.\n        configure_optimizers():\n            Setup Adam optimizer and ReduceLROnPlateau scheduler.\n        on_fit_start():\n            Save normalization statistics at the beginning of training.\n        on_test_end():\n            Collect test metrics across datasets and export summary CSV reports.\n\n    Notes:\n        - Node types are distinguished using the global constants (`PQ`, `PV`, `REF`).\n        - The datamodule must provide `batch.mask` for masking node features.\n        - Test metrics include per-node-type RMSE for [Pd, Qd, Pg, Qg, Vm, Va].\n        - Reports are saved under `&lt;mlflow_artifacts&gt;/test/&lt;dataset&gt;.csv`.\n\n    Example:\n        ```python\n        model = FeatureReconstructionTask(args, node_normalizers, edge_normalizers)\n        output = model(batch.x, batch.pe, batch.edge_index, batch.edge_attr, batch.batch)\n        ```\n    \"\"\"\n\n    def __init__(self, args, node_normalizers, edge_normalizers):\n        super().__init__()\n        self.model = load_model(args=args)\n        self.args = args\n        self.loss_fn = get_loss_function(args)\n        self.batch_size = int(args.training.batch_size)\n        self.node_normalizers = node_normalizers\n        self.edge_normalizers = edge_normalizers\n        self.save_hyperparameters()\n\n    def forward(self, x, pe, edge_index, edge_attr, batch, mask=None):\n        if mask is not None:\n            mask_value_expanded = self.model.mask_value.expand(x.shape[0], -1)\n            x[:, : mask.shape[1]][mask] = mask_value_expanded[mask]\n        return self.model(x, pe, edge_index, edge_attr, batch)\n\n    @rank_zero_only\n    def on_fit_start(self):\n        # Determine save path\n        if isinstance(self.logger, MLFlowLogger):\n            log_dir = os.path.join(\n                self.logger.save_dir,\n                self.logger.experiment_id,\n                self.logger.run_id,\n                \"artifacts\",\n                \"stats\",\n            )\n        else:\n            log_dir = os.path.join(self.logger.save_dir, \"stats\")\n\n        os.makedirs(log_dir, exist_ok=True)\n        log_stats_path = os.path.join(log_dir, \"normalization_stats.txt\")\n\n        # Collect normalization stats\n        with open(log_stats_path, \"w\") as log_file:\n            for i, normalizer in enumerate(self.node_normalizers):\n                log_file.write(\n                    f\"Node Normalizer {self.args.data.networks[i]} stats:\\n{normalizer.get_stats()}\\n\\n\",\n                )\n\n            for i, normalizer in enumerate(self.edge_normalizers):\n                log_file.write(\n                    f\"Edge Normalizer {self.args.data.networks[i]} stats:\\n{normalizer.get_stats()}\\n\\n\",\n                )\n\n    def shared_step(self, batch):\n        output = self.forward(\n            x=batch.x,\n            pe=batch.pe,\n            edge_index=batch.edge_index,\n            edge_attr=batch.edge_attr,\n            batch=batch.batch,\n            mask=batch.mask,\n        )\n\n        loss_dict = self.loss_fn(\n            output,\n            batch.y,\n            batch.edge_index,\n            batch.edge_attr,\n            batch.mask,\n        )\n        return output, loss_dict\n\n    def training_step(self, batch):\n        _, loss_dict = self.shared_step(batch)\n        current_lr = self.optimizer.param_groups[0][\"lr\"]\n        metrics = {}\n        metrics[\"Training Loss\"] = loss_dict[\"loss\"].detach()\n        metrics[\"Learning Rate\"] = current_lr\n        for metric, value in metrics.items():\n            self.log(\n                metric,\n                value,\n                batch_size=batch.num_graphs,\n                sync_dist=True,\n                on_epoch=True,\n                prog_bar=True,\n                logger=True,\n                on_step=False,\n            )\n\n        return loss_dict[\"loss\"]\n\n    def validation_step(self, batch, batch_idx):\n        _, loss_dict = self.shared_step(batch)\n        loss_dict[\"loss\"] = loss_dict[\"loss\"].detach()\n        for metric, value in loss_dict.items():\n            metric_name = f\"Validation {metric}\"\n            self.log(\n                metric_name,\n                value,\n                batch_size=batch.num_graphs,\n                sync_dist=True,\n                on_epoch=True,\n                prog_bar=True,\n                logger=True,\n                on_step=False,\n            )\n\n        return loss_dict[\"loss\"]\n\n    def test_step(self, batch, batch_idx, dataloader_idx=0):\n        output, loss_dict = self.shared_step(batch)\n\n        dataset_name = self.args.data.networks[dataloader_idx]\n\n        output_denorm = self.node_normalizers[dataloader_idx].inverse_transform(output)\n        target_denorm = self.node_normalizers[dataloader_idx].inverse_transform(batch.y)\n\n        mask_PQ = batch.x[:, PQ] == 1\n        mask_PV = batch.x[:, PV] == 1\n        mask_REF = batch.x[:, REF] == 1\n\n        mse_PQ = F.mse_loss(\n            output_denorm[mask_PQ],\n            target_denorm[mask_PQ],\n            reduction=\"none\",\n        )\n        mse_PV = F.mse_loss(\n            output_denorm[mask_PV],\n            target_denorm[mask_PV],\n            reduction=\"none\",\n        )\n        mse_REF = F.mse_loss(\n            output_denorm[mask_REF],\n            target_denorm[mask_REF],\n            reduction=\"none\",\n        )\n\n        mse_PQ = mse_PQ.mean(dim=0)\n        mse_PV = mse_PV.mean(dim=0)\n        mse_REF = mse_REF.mean(dim=0)\n\n        loss_dict[\"MSE PQ nodes - PD\"] = mse_PQ[PD]\n        loss_dict[\"MSE PV nodes - PD\"] = mse_PV[PD]\n        loss_dict[\"MSE REF nodes - PD\"] = mse_REF[PD]\n\n        loss_dict[\"MSE PQ nodes - QD\"] = mse_PQ[QD]\n        loss_dict[\"MSE PV nodes - QD\"] = mse_PV[QD]\n        loss_dict[\"MSE REF nodes - QD\"] = mse_REF[QD]\n\n        loss_dict[\"MSE PQ nodes - PG\"] = mse_PQ[PG]\n        loss_dict[\"MSE PV nodes - PG\"] = mse_PV[PG]\n        loss_dict[\"MSE REF nodes - PG\"] = mse_REF[PG]\n\n        loss_dict[\"MSE PQ nodes - QG\"] = mse_PQ[QG]\n        loss_dict[\"MSE PV nodes - QG\"] = mse_PV[QG]\n        loss_dict[\"MSE REF nodes - QG\"] = mse_REF[QG]\n\n        loss_dict[\"MSE PQ nodes - VM\"] = mse_PQ[VM]\n        loss_dict[\"MSE PV nodes - VM\"] = mse_PV[VM]\n        loss_dict[\"MSE REF nodes - VM\"] = mse_REF[VM]\n\n        loss_dict[\"MSE PQ nodes - VA\"] = mse_PQ[VA]\n        loss_dict[\"MSE PV nodes - VA\"] = mse_PV[VA]\n        loss_dict[\"MSE REF nodes - VA\"] = mse_REF[VA]\n\n        loss_dict[\"Test loss\"] = loss_dict.pop(\"loss\").detach()\n        for metric, value in loss_dict.items():\n            metric_name = f\"{dataset_name}/{metric}\"\n            if \"p.u.\" in metric:\n                # Denormalize metrics expressed in p.u.\n                value *= self.node_normalizers[dataloader_idx].baseMVA\n                metric_name = metric_name.replace(\"in p.u.\", \"\").strip()\n            self.log(\n                metric_name,\n                value,\n                batch_size=batch.num_graphs,\n                add_dataloader_idx=False,\n                sync_dist=True,\n                logger=False,\n            )\n        return\n\n    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n        output, _ = self.shared_step(batch)\n        output_denorm = self.node_normalizers[dataloader_idx].inverse_transform(output)\n\n        # Count buses and generate per-node scenario_id\n        bus_counts = batch.batch.unique(return_counts=True)[1]\n        scenario_ids = batch.scenario_id  # shape: [num_graphs]\n        scenario_per_node = torch.cat(\n            [\n                torch.full((count,), sid, dtype=torch.int32)\n                for count, sid in zip(bus_counts, scenario_ids)\n            ],\n        )\n\n        bus_numbers = np.concatenate([np.arange(count.item()) for count in bus_counts])\n\n        return {\n            \"output\": output_denorm.cpu().numpy(),\n            \"scenario_id\": scenario_per_node,\n            \"bus_number\": bus_numbers,\n        }\n\n    @rank_zero_only\n    def on_test_end(self):\n        if isinstance(self.logger, MLFlowLogger):\n            artifact_dir = os.path.join(\n                self.logger.save_dir,\n                self.logger.experiment_id,\n                self.logger.run_id,\n                \"artifacts\",\n            )\n        else:\n            artifact_dir = self.logger.save_dir\n\n        final_metrics = self.trainer.callback_metrics\n        grouped_metrics = {}\n\n        for full_key, value in final_metrics.items():\n            try:\n                value = value.item()\n            except AttributeError:\n                pass\n\n            if \"/\" in full_key:\n                dataset_name, metric = full_key.split(\"/\", 1)\n                if dataset_name not in grouped_metrics:\n                    grouped_metrics[dataset_name] = {}\n                grouped_metrics[dataset_name][metric] = value\n\n        for dataset, metrics in grouped_metrics.items():\n            rmse_PQ = [\n                metrics.get(f\"MSE PQ nodes - {label}\", float(\"nan\")) ** 0.5\n                for label in [\"PD\", \"QD\", \"PG\", \"QG\", \"VM\", \"VA\"]\n            ]\n            rmse_PV = [\n                metrics.get(f\"MSE PV nodes - {label}\", float(\"nan\")) ** 0.5\n                for label in [\"PD\", \"QD\", \"PG\", \"QG\", \"VM\", \"VA\"]\n            ]\n            rmse_REF = [\n                metrics.get(f\"MSE REF nodes - {label}\", float(\"nan\")) ** 0.5\n                for label in [\"PD\", \"QD\", \"PG\", \"QG\", \"VM\", \"VA\"]\n            ]\n\n            avg_active_res = metrics.get(\"Active Power Loss\", \" \")\n            avg_reactive_res = metrics.get(\"Reactive Power Loss\", \" \")\n\n            data = {\n                \"Metric\": [\n                    \"RMSE-PQ\",\n                    \"RMSE-PV\",\n                    \"RMSE-REF\",\n                    \"Avg. active res. (MW)\",\n                    \"Avg. reactive res. (MVar)\",\n                ],\n                \"Pd (MW)\": [\n                    rmse_PQ[0],\n                    rmse_PV[0],\n                    rmse_REF[0],\n                    avg_active_res,\n                    avg_reactive_res,\n                ],\n                \"Qd (MVar)\": [rmse_PQ[1], rmse_PV[1], rmse_REF[1], \" \", \" \"],\n                \"Pg (MW)\": [rmse_PQ[2], rmse_PV[2], rmse_REF[2], \" \", \" \"],\n                \"Qg (MVar)\": [rmse_PQ[3], rmse_PV[3], rmse_REF[3], \" \", \" \"],\n                \"Vm (p.u.)\": [rmse_PQ[4], rmse_PV[4], rmse_REF[4], \" \", \" \"],\n                \"Va (degree)\": [rmse_PQ[5], rmse_PV[5], rmse_REF[5], \" \", \" \"],\n            }\n\n            df = pd.DataFrame(data)\n\n            test_dir = os.path.join(artifact_dir, \"test\")\n            os.makedirs(test_dir, exist_ok=True)\n            csv_path = os.path.join(test_dir, f\"{dataset}.csv\")\n            df.to_csv(csv_path, index=False)\n\n    def configure_optimizers(self):\n        self.optimizer = torch.optim.Adam(\n            self.model.parameters(),\n            lr=self.args.optimizer.learning_rate,\n            betas=(self.args.optimizer.beta1, self.args.optimizer.beta2),\n        )\n\n        self.scheduler = ReduceLROnPlateau(\n            self.optimizer,\n            mode=\"min\",\n            factor=self.args.optimizer.lr_decay,\n            patience=self.args.optimizer.lr_patience,\n        )\n        config_optim = {\n            \"optimizer\": self.optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": self.scheduler,\n                \"monitor\": \"Validation loss\",\n                \"reduce_on_plateau\": True,\n            },\n        }\n        return config_optim\n</code></pre>"},{"location":"training/loss/","title":"Loss Functions","text":""},{"location":"training/loss/#power-balance-equation-loss","title":"<code>Power Balance Equation Loss</code>","text":"\\[ \\mathcal{L}_{\\text{PBE}} = \\frac{1}{N} \\sum_{i=1}^N \\left| (P_{G,i} - P_{D,i}) + j(Q_{G,i} - Q_{D,i}) - S_{\\text{injection}, i} \\right| \\] <p>               Bases: <code>Module</code></p> <p>Loss based on the Power Balance Equations.</p> Source code in <code>gridfm_graphkit/training/loss.py</code> <pre><code>class PBELoss(nn.Module):\n    \"\"\"\n    Loss based on the Power Balance Equations.\n    \"\"\"\n\n    def __init__(self, visualization=False):\n        super(PBELoss, self).__init__()\n\n        self.visualization = visualization\n\n    def forward(self, pred, target, edge_index, edge_attr, mask):\n        # Create a temporary copy of pred to avoid modifying it\n        temp_pred = pred.clone()\n\n        # If a value is not masked, then use the original one\n        unmasked = ~mask\n        temp_pred[unmasked] = target[unmasked]\n\n        # Voltage magnitudes and angles\n        V_m = temp_pred[:, VM]  # Voltage magnitudes\n        V_a = temp_pred[:, VA]  # Voltage angles\n\n        # Compute the complex voltage vector V\n        V = V_m * torch.exp(1j * V_a)\n\n        # Compute the conjugate of V\n        V_conj = torch.conj(V)\n\n        # Extract edge attributes for Y_bus\n        edge_complex = edge_attr[:, G] + 1j * edge_attr[:, B]\n\n        # Construct sparse admittance matrix (real and imaginary parts separately)\n        Y_bus_sparse = to_torch_coo_tensor(\n            edge_index,\n            edge_complex,\n            size=(target.size(0), target.size(0)),\n        )\n\n        # Conjugate of the admittance matrix\n        Y_bus_conj = torch.conj(Y_bus_sparse)\n\n        # Compute the complex power injection S_injection\n        S_injection = torch.diag(V) @ Y_bus_conj @ V_conj\n\n        # Compute net power balance\n        net_P = temp_pred[:, PG] - temp_pred[:, PD]\n        net_Q = temp_pred[:, QG] - temp_pred[:, QD]\n        S_net_power_balance = net_P + 1j * net_Q\n\n        # Power balance loss\n        loss = torch.mean(\n            torch.abs(S_net_power_balance - S_injection),\n        )  # Mean of absolute complex power value\n\n        real_loss_power = torch.mean(\n            torch.abs(torch.real(S_net_power_balance - S_injection)),\n        )\n        imag_loss_power = torch.mean(\n            torch.abs(torch.imag(S_net_power_balance - S_injection)),\n        )\n        if self.visualization:\n            return {\n                \"loss\": loss,\n                \"Power loss in p.u.\": loss.detach(),\n                \"Active Power Loss in p.u.\": real_loss_power.detach(),\n                \"Reactive Power Loss in p.u.\": imag_loss_power.detach(),\n                \"Nodal Active Power Loss in p.u.\": torch.abs(\n                    torch.real(S_net_power_balance - S_injection),\n                ),\n                \"Nodal Reactive Power Loss in p.u.\": torch.abs(\n                    torch.imag(S_net_power_balance - S_injection),\n                ),\n            }\n        else:\n            return {\n                \"loss\": loss,\n                \"Power loss in p.u.\": loss.detach(),\n                \"Active Power Loss in p.u.\": real_loss_power.detach(),\n                \"Reactive Power Loss in p.u.\": imag_loss_power.detach(),\n            }\n</code></pre>"},{"location":"training/loss/#mean-squared-error-loss","title":"<code>Mean Squared Error Loss</code>","text":"\\[ \\mathcal{L}_{\\text{MSE}} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 \\] <p>               Bases: <code>Module</code></p> <p>Standard Mean Squared Error loss.</p> Source code in <code>gridfm_graphkit/training/loss.py</code> <pre><code>class MSELoss(nn.Module):\n    \"\"\"Standard Mean Squared Error loss.\"\"\"\n\n    def __init__(self, reduction=\"mean\"):\n        super(MSELoss, self).__init__()\n        self.reduction = reduction\n\n    def forward(self, pred, target, edge_index=None, edge_attr=None, mask=None):\n        loss = F.mse_loss(pred, target, reduction=self.reduction)\n        return {\"loss\": loss, \"MSE loss\": loss.detach()}\n</code></pre>"},{"location":"training/loss/#masked-mean-squared-error-loss","title":"<code>Masked Mean Squared Error Loss</code>","text":"\\[ \\mathcal{L}_{\\text{MaskedMSE}} = \\frac{1}{|M|} \\sum_{i \\in M} (y_i - \\hat{y}_i)^2 \\] <p>               Bases: <code>Module</code></p> <p>Mean Squared Error loss computed only on masked elements.</p> Source code in <code>gridfm_graphkit/training/loss.py</code> <pre><code>class MaskedMSELoss(nn.Module):\n    \"\"\"\n    Mean Squared Error loss computed only on masked elements.\n    \"\"\"\n\n    def __init__(self, reduction=\"mean\"):\n        super(MaskedMSELoss, self).__init__()\n        self.reduction = reduction\n\n    def forward(self, pred, target, edge_index=None, edge_attr=None, mask=None):\n        loss = F.mse_loss(pred[mask], target[mask], reduction=self.reduction)\n        return {\"loss\": loss, \"Masked MSE loss\": loss.detach()}\n</code></pre>"},{"location":"training/loss/#scaled-cosine-error-loss","title":"<code>Scaled Cosine Error Loss</code>","text":"\\[ \\mathcal{L}_{\\text{SCE}} = \\frac{1}{N} \\sum_{i=1}^N \\left(1 - \\frac{\\hat{y}^T_i \\cdot y_i}{\\|\\hat{y}_i\\| \\|y_i\\|}\\right)^\\alpha \\text{ , } \\alpha \\geq 1 \\] <p>               Bases: <code>Module</code></p> <p>Scaled Cosine Error Loss with optional masking and normalization.</p> Source code in <code>gridfm_graphkit/training/loss.py</code> <pre><code>class SCELoss(nn.Module):\n    \"\"\"Scaled Cosine Error Loss with optional masking and normalization.\"\"\"\n\n    def __init__(self, alpha=3):\n        super(SCELoss, self).__init__()\n        self.alpha = alpha\n\n    def forward(self, pred, target, edge_index=None, edge_attr=None, mask=None):\n        if mask is not None:\n            pred = F.normalize(pred[mask], p=2, dim=-1)\n            target = F.normalize(target[mask], p=2, dim=-1)\n        else:\n            pred = F.normalize(pred, p=2, dim=-1)\n            target = F.normalize(target, p=2, dim=-1)\n\n        loss = ((1 - (pred * target).sum(dim=-1)).pow(self.alpha)).mean()\n\n        return {\n            \"loss\": loss,\n            \"SCE loss\": loss.detach(),\n        }\n</code></pre>"},{"location":"training/loss/#mixed-loss","title":"<code>Mixed Loss</code>","text":"\\[ \\mathcal{L}_{\\text{Mixed}} = \\sum_{m=1}^M w_m \\cdot \\mathcal{L}_m \\] <p>               Bases: <code>Module</code></p> <p>Combines multiple loss functions with weighted sum.</p> <p>Parameters:</p> Name Type Description Default <code>loss_functions</code> <code>list[Module]</code> <p>List of loss functions.</p> required <code>weights</code> <code>list[float]</code> <p>Corresponding weights for each loss function.</p> required Source code in <code>gridfm_graphkit/training/loss.py</code> <pre><code>class MixedLoss(nn.Module):\n    \"\"\"\n    Combines multiple loss functions with weighted sum.\n\n    Args:\n        loss_functions (list[nn.Module]): List of loss functions.\n        weights (list[float]): Corresponding weights for each loss function.\n    \"\"\"\n\n    def __init__(self, loss_functions, weights):\n        super(MixedLoss, self).__init__()\n\n        if len(loss_functions) != len(weights):\n            raise ValueError(\n                \"The number of loss functions must match the number of weights.\",\n            )\n\n        self.loss_functions = nn.ModuleList(loss_functions)\n        self.weights = weights\n\n    def forward(self, pred, target, edge_index=None, edge_attr=None, mask=None):\n        \"\"\"\n        Compute the weighted sum of all specified losses.\n\n        Parameters:\n\n        - pred: Predictions.\n        - target: Ground truth.\n        - edge_index: Optional edge index for graph-based losses.\n        - edge_attr: Optional edge attributes for graph-based losses.\n        - mask: Optional mask to filter the inputs for certain losses.\n\n        Returns:\n        - A dictionary with the total loss and individual losses.\n        \"\"\"\n        total_loss = 0.0\n        loss_details = {}\n\n        for i, loss_fn in enumerate(self.loss_functions):\n            loss_output = loss_fn(\n                pred,\n                target,\n                edge_index=edge_index,\n                edge_attr=edge_attr,\n                mask=mask,\n            )\n\n            # Assume each loss function returns a dictionary with a \"loss\" key\n            individual_loss = loss_output.pop(\"loss\")\n            weighted_loss = self.weights[i] * individual_loss\n\n            total_loss += weighted_loss\n\n            # Add other keys from the loss output to the details\n            for key, val in loss_output.items():\n                loss_details[key] = val\n\n        loss_details[\"loss\"] = total_loss\n        return loss_details\n</code></pre> <code>forward(pred, target, edge_index=None, edge_attr=None, mask=None)</code> \u00b6 <p>Compute the weighted sum of all specified losses.</p> <p>Parameters:</p> <ul> <li>pred: Predictions.</li> <li>target: Ground truth.</li> <li>edge_index: Optional edge index for graph-based losses.</li> <li>edge_attr: Optional edge attributes for graph-based losses.</li> <li>mask: Optional mask to filter the inputs for certain losses.</li> </ul> <p>Returns: - A dictionary with the total loss and individual losses.</p> Source code in <code>gridfm_graphkit/training/loss.py</code> <pre><code>def forward(self, pred, target, edge_index=None, edge_attr=None, mask=None):\n    \"\"\"\n    Compute the weighted sum of all specified losses.\n\n    Parameters:\n\n    - pred: Predictions.\n    - target: Ground truth.\n    - edge_index: Optional edge index for graph-based losses.\n    - edge_attr: Optional edge attributes for graph-based losses.\n    - mask: Optional mask to filter the inputs for certain losses.\n\n    Returns:\n    - A dictionary with the total loss and individual losses.\n    \"\"\"\n    total_loss = 0.0\n    loss_details = {}\n\n    for i, loss_fn in enumerate(self.loss_functions):\n        loss_output = loss_fn(\n            pred,\n            target,\n            edge_index=edge_index,\n            edge_attr=edge_attr,\n            mask=mask,\n        )\n\n        # Assume each loss function returns a dictionary with a \"loss\" key\n        individual_loss = loss_output.pop(\"loss\")\n        weighted_loss = self.weights[i] * individual_loss\n\n        total_loss += weighted_loss\n\n        # Add other keys from the loss output to the details\n        for key, val in loss_output.items():\n            loss_details[key] = val\n\n    loss_details[\"loss\"] = total_loss\n    return loss_details\n</code></pre>"},{"location":"tutorials/contingency_analysis/","title":"Contingency analysis","text":"<p>\ud83d\udc49 Link to the tutorial on Google Colab</p> <p>Contingency analysis is a critical process in power system operations used to assess the impact of potential failures (e.g., line outages) on grid stability and reliability. It helps operators prepare for unexpected events by simulating scenarios such as N-1 or N-2 contingencies, where one or more components are removed from service. This analysis ensures that the grid can continue to operate within safe limits even under stressed conditions.</p>"},{"location":"tutorials/contingency_analysis/#dataset-generation-and-model-evaluation","title":"Dataset Generation and Model Evaluation","text":"<p>The dataset used in this study originates from the Texas transmission grid, which includes approximately 2,000 nodes. Using the contingency mode of the <code>gridfm-datakit</code>, we simulated N-2 contingencies by removing up to two transmission lines at a time. For each scenario, we first solved the optimal power flow (OPF) problem to determine the generation dispatch. Then, we applied the contingency by removing lines and re-solved the power flow to observe the resulting grid state.</p> <p>This process generated around 100,000 unique scenarios. Our model, GridFMv0.1, was fine-tuned on this dataset to predict power flow outcomes. For demonstration purposes, we selected a subsample of 10 scenarios. The <code>gridfm-datakit</code> also computed DC power flow results, enabling a comparison between GridFMv0.1 predictions and traditional DC power flow estimates, specifically in terms of line loading accuracy.</p> <p>All predictions are benchmarked against the ground truth obtained from AC power flow simulations. Additionally, we analyze bus voltage violations, which GridFM can predict but are not captured by the DC solver, highlighting GridFM\u2019s enhanced capabilities in modeling grid behavior.</p> <pre><code>import sys\n\nif \"google.colab\" in sys.modules:\n    try:\n        !git clone https://github.com/gridfm/gridfm-graphkit.git\n        %cd /content/gridfm-graphkit\n        !pip install .\n        %cd examples/notebooks/\n    except Exception as e:\n        print(f\"Failed to start Google Collab setup, due to {e}\")\n</code></pre> <pre><code>from gridfm_graphkit.datasets.postprocessing import (\n    compute_branch_currents_kA,\n    compute_loading,\n)\nfrom gridfm_graphkit.datasets.postprocessing import create_admittance_matrix\nfrom gridfm_graphkit.utils.utils import compute_cm_metrics\nfrom gridfm_graphkit.utils.visualization import (\n    plot_mass_correlation_density,\n    plot_cm,\n    plot_loading_predictions,\n    plot_mass_correlation_density_voltage,\n)\n\nimport os\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score\nimport numpy as np\nimport pandas as pd\n</code></pre>"},{"location":"tutorials/contingency_analysis/#load-data","title":"Load Data","text":"<p>We load both the ground truth and predicted values of the power flow solution. The predictions are generated using the <code>gridfm-graphkit</code> CLI:</p> <pre><code>gridfm-graphkit predict ...\n</code></pre> <p>We then merge the datasets using <code>scenario</code> and <code>bus</code> as keys, allowing us to align the predicted and actual values for each grid state and bus.</p> <pre><code>root_pred_folder = \"../data/contingency_texas/\"\nprediction_dir = \"prediction_gridfm01\"\nlabel_plot = \"GridFM_v0.1 Fine-tuned\"\n\npf_node_GT = pd.read_csv(os.path.join(root_pred_folder, \"pf_node_10_examples.csv\"))\npg_node_predicted = pd.read_csv(\n    os.path.join(root_pred_folder, \"predictions_10_examples.csv\")\n)\n\nbranch_idx_removed = pd.read_csv(\"{}branch_idx_removed.csv\".format(root_pred_folder))\nedge_params = pd.read_csv(\"{}edge_params.csv\".format(root_pred_folder))\nbus_params = pd.read_csv(\"{}bus_params.csv\".format(root_pred_folder))\n\npf_node = pg_node_predicted.merge(pf_node_GT, on=[\"scenario\", \"bus\"], how=\"left\")\n</code></pre>"},{"location":"tutorials/contingency_analysis/#create-admittance-matrix","title":"Create Admittance matrix","text":"<pre><code>sn_mva = 100\nYf, Yt, Vf_base_kV, Vt_base_kV = create_admittance_matrix(\n    bus_params, edge_params, sn_mva\n)\nrate_a = edge_params[\"rate_a\"]\n</code></pre>"},{"location":"tutorials/contingency_analysis/#correct-voltage-predictions-for-gridfm-and-dc","title":"Correct voltage predictions for GridFM and DC","text":"<pre><code>pf_node[\"Vm_pred_corrected\"] = pf_node[\"VM_pred\"]\npf_node[\"Va_pred_corrected\"] = pf_node[\"VA_pred\"]\n\npf_node.loc[pf_node.PV == 1, \"Vm_pred_corrected\"] = pf_node.loc[pf_node.PV == 1, \"Vm\"]\npf_node.loc[pf_node.REF == 1, \"Va_pred_corrected\"] = pf_node.loc[pf_node.REF == 1, \"Va\"]\n\npf_node[\"Vm_dc_corrected\"] = pf_node[\"Vm_dc\"]\npf_node[\"Va_dc_corrected\"] = pf_node[\"Va_dc\"]\n\npf_node.loc[pf_node.PV == 1, \"Vm_dc_corrected\"] = pf_node.loc[pf_node.PV == 1, \"Vm\"]\npf_node.loc[pf_node.REF == 1, \"Va_dc_corrected\"] = pf_node.loc[pf_node.REF == 1, \"Va\"]\n</code></pre>"},{"location":"tutorials/contingency_analysis/#compute-branch-current-and-line-loading","title":"Compute branch current and line loading","text":"<pre><code>loadings = []\nloadings_pred = []\nloadings_dc = []\n\nfor scenario_idx in tqdm(pf_node.scenario.unique()):\n    pf_node_scenario = pf_node[pf_node.scenario == scenario_idx]\n    branch_idx_removed_scenario = (\n        branch_idx_removed[branch_idx_removed.scenario == scenario_idx]\n        .iloc[:, 1:]\n        .values\n    )\n    # remove nan\n    branch_idx_removed_scenario = branch_idx_removed_scenario[\n        ~np.isnan(branch_idx_removed_scenario)\n    ].astype(np.int32)\n    V_true = pf_node_scenario[\"Vm\"].values * np.exp(\n        1j * pf_node_scenario[\"Va\"].values * np.pi / 180\n    )\n    V_pred = pf_node_scenario[\"Vm_pred_corrected\"].values * np.exp(\n        1j * pf_node_scenario[\"Va_pred_corrected\"].values * np.pi / 180\n    )\n    V_dc = pf_node_scenario[\"Vm_dc_corrected\"].values * np.exp(\n        1j * pf_node_scenario[\"Va_dc_corrected\"].values * np.pi / 180\n    )\n    If_true, It_true = compute_branch_currents_kA(\n        Yf, Yt, V_true, Vf_base_kV, Vt_base_kV, sn_mva\n    )\n    If_pred, It_pred = compute_branch_currents_kA(\n        Yf, Yt, V_pred, Vf_base_kV, Vt_base_kV, sn_mva\n    )\n    If_dc, It_dc = compute_branch_currents_kA(\n        Yf, Yt, V_dc, Vf_base_kV, Vt_base_kV, sn_mva\n    )\n\n    loading_true = compute_loading(If_true, It_true, Vf_base_kV, Vt_base_kV, rate_a)\n    loading_pred = compute_loading(If_pred, It_pred, Vf_base_kV, Vt_base_kV, rate_a)\n    loading_dc = compute_loading(If_dc, It_dc, Vf_base_kV, Vt_base_kV, rate_a)\n\n    # remove the branches that are removed from loading\n    loading_true[branch_idx_removed_scenario] = -1\n    loading_pred[branch_idx_removed_scenario] = -1\n    loading_dc[branch_idx_removed_scenario] = -1\n\n    loadings.append(loading_true)\n    loadings_pred.append(loading_pred)\n    loadings_dc.append(loading_dc)\n\n\nloadings = np.array(loadings)\nloadings_pred = np.array(loadings_pred)\nloadings_dc = np.array(loadings_dc)\nremoved_lines = loadings == -1\nremoved_lines_pred = loadings_pred == -1\nremoved_lines_dc = loadings_dc == -1\n\n\n# assert the same lines are removed\nassert (removed_lines == removed_lines_pred).all()\nassert (removed_lines == removed_lines_dc).all()\n\n# assert the same number of lines are removed\nassert removed_lines.sum() == removed_lines_pred.sum()\nassert removed_lines.sum() == removed_lines_dc.sum()\n\noverloadings = loadings[removed_lines == False] &gt; 1.0\noverloadings_pred = loadings_pred[removed_lines == False] &gt; 1.0\noverloadings_dc = loadings_dc[removed_lines == False] &gt; 1.0\n</code></pre>"},{"location":"tutorials/contingency_analysis/#histogram-of-true-line-loadings","title":"Histogram of true line loadings","text":"<pre><code>plt.hist(loadings[removed_lines == False], bins=100)\nplt.xlabel(\"Line Loadings\")\nplt.ylabel(\"Frequency\")\n# log scale\nplt.savefig(f\"loadings_histogram_{prediction_dir}.png\")\nplt.show()\n</code></pre>"},{"location":"tutorials/contingency_analysis/#predicted-vs-true-line-loading","title":"Predicted vs True line loading","text":"<pre><code># Valid lines\nvalid_mask = removed_lines == False\n\n# Extract valid values\ntrue_vals = loadings[valid_mask]\ngfm_vals = loadings_pred[valid_mask]\ndc_vals = loadings_dc[valid_mask]\n</code></pre> <pre><code>plot_mass_correlation_density(true_vals, gfm_vals, prediction_dir, label_plot)\n</code></pre> <pre><code>plot_mass_correlation_density(true_vals, dc_vals, \"DC\", \"DC Solver\")\n</code></pre> <pre><code>plot_cm(TN_gridfm, FP_gridfm, FN_gridfm, TP_gridfm, prediction_dir, label_plot)\n</code></pre> <pre><code>plot_cm(TN_dc, FP_dc, FN_dc, TP_dc, \"DC\", \"DC Solver\")\n</code></pre> <pre><code># Histograms of loadings\nplot_loading_predictions(\n    loadings_pred[removed_lines == False],\n    loadings_dc[removed_lines == False],\n    loadings[removed_lines == False],\n    prediction_dir,\n    label_plot,\n)\n</code></pre> <pre><code># create df from loadings\nloadings_df = pd.DataFrame(loadings)\nloadings_df.columns = [f\"branch_{i}\" for i in range(loadings_df.shape[1])]\n\nloadings_pred_df = pd.DataFrame(loadings_pred)\nloadings_pred_df.columns = [f\"branch_{i}\" for i in range(loadings_pred_df.shape[1])]\n\nloadings_df[\"scenario\"] = pf_node[\"scenario\"].unique()\nloadings_pred_df[\"scenario\"] = pf_node[\"scenario\"].unique()\n\n# make bar plot of wrongly classified loadings for different bins\nbins = np.arange(0, 2.2, 0.2)\nmse_pred = []\nmse_dc = []\nfor i in range(len(bins) - 1):\n    idx_in_bins = (loadings[removed_lines == False] &gt; bins[i]) &amp; (\n        loadings[removed_lines == False] &lt; bins[i + 1]\n    )\n    mse_pred.append(\n        np.mean(\n            (\n                loadings_pred[removed_lines == False][idx_in_bins]\n                - loadings[removed_lines == False][idx_in_bins]\n            )\n            ** 2\n        )\n    )\n    mse_dc.append(\n        np.mean(\n            (\n                loadings_dc[removed_lines == False][idx_in_bins]\n                - loadings[removed_lines == False][idx_in_bins]\n            )\n            ** 2\n        )\n    )\n\n\n# labels\nlabels = [f\"{bins[i]:.1f}-{bins[i + 1]:.1f}\" for i in range(len(bins) - 1)]\nplt.bar(labels, mse_pred, label=label_plot, alpha=0.5)\nplt.bar(labels, mse_dc, label=\"DC\", alpha=0.5)\nplt.legend()\nplt.xlabel(\"Loadings\")\nplt.ylabel(\"MSE\")\n# y log scale\nplt.yscale(\"log\")\n# rotate x labels\nplt.xticks(rotation=45)\nplt.savefig(f\"loading_mse_{prediction_dir}.png\")\nplt.show()\n</code></pre>"},{"location":"tutorials/contingency_analysis/#voltage-violations","title":"Voltage violations","text":"<pre><code># merge bus_params[\"vmax\"] and bus_params[\"vmin\"] with pf_node on bus_idx\npf_node = pd.merge(pf_node, bus_params[[\"bus\", \"vmax\", \"vmin\"]], on=\"bus\", how=\"left\")\n\nplot_mass_correlation_density_voltage(pf_node, prediction_dir, label_plot)\n</code></pre>"},{"location":"tutorials/feature_reconstruction/","title":"Visualizing predictions of GridFM","text":"<p>\ud83d\udc49 Link to the tutorial on Google Colab</p> <p>This notebook demonstrates the state reconstruction capabilities of GridFM-v0.2, a graph-based neural network model for transmission grids. We focus on the IEEE case30 network, a standard benchmark with 30 buses, chosen for its compact size and suitability for visualization.</p> <p>The dataset includes 1,023 load scenarios, each representing a different operating condition of the grid. For each scenario, the model reconstructs the six key features of the power flow solution that are masked:</p> <ul> <li>Active Power Demand (MW)</li> <li>Reactive Power Demand (MVar)</li> <li>Active Power Generated (MW)</li> <li>Reactive Power Generated (MVar)</li> <li>Voltage Magnitude (p.u.)</li> <li>Voltage Angle (degrees)</li> </ul> <pre><code>import sys\n\nif \"google.colab\" in sys.modules:\n    try:\n        !git clone https://github.com/gridfm/gridfm-graphkit.git\n        %cd /content/gridfm-graphkit\n        !pip install .\n        %cd examples/notebooks/\n    except Exception as e:\n        print(f\"Failed to start Google Collab setup, due to {e}\")\n</code></pre> <pre><code>from gridfm_graphkit.datasets.powergrid_datamodule import LitGridDataModule\nfrom gridfm_graphkit.io.param_handler import NestedNamespace\nfrom gridfm_graphkit.tasks.feature_reconstruction_task import FeatureReconstructionTask\nfrom gridfm_graphkit.utils.visualization import visualize_error, visualize_quantity_heatmap\nfrom gridfm_graphkit.datasets.globals import PD, QD, PG, QG, VM, VA\n\nimport yaml\nimport torch\nimport numpy as np\nimport random\n</code></pre>"},{"location":"tutorials/feature_reconstruction/#load-yaml-configuration-file","title":"Load YAML configuration file","text":"<pre><code>config_path = \"../config/case30_ieee_base.yaml\"\nwith open(config_path) as f:\n    config_dict = yaml.safe_load(f)\n\nconfig_args = NestedNamespace(**config_dict)\ntorch.manual_seed(config_args.seed)\nrandom.seed(config_args.seed)\nnp.random.seed(config_args.seed)\n</code></pre>"},{"location":"tutorials/feature_reconstruction/#initialize-the-datamodule","title":"Initialize the DataModule","text":"<pre><code>data_module = LitGridDataModule(config_args, \"../data\")\ndata_module.setup(\"test\")\ntest_loader = data_module.test_dataloader()\n</code></pre>"},{"location":"tutorials/feature_reconstruction/#load-the-pre-trained-model-gridfm-v02","title":"Load the pre-trained model GridFM-v0.2","text":"<pre><code>model = FeatureReconstructionTask(\n    config_args, data_module.node_normalizers, data_module.edge_normalizers\n)\nstate_dict = torch.load(\"../models/GridFM_v0_2.pth\")\nmodel.load_state_dict(state_dict)\n</code></pre>"},{"location":"tutorials/feature_reconstruction/#perform-inference-batch-size-is-equal-to-1-for-further-visualization-purposes","title":"Perform inference, batch size is equal to 1 for further visualization purposes","text":"<pre><code>batch = next(iter(test_loader[0]))\n\nmodel.eval()\nwith torch.no_grad():\n    output = model(\n        x=batch.x,\n        pe=batch.pe,\n        edge_index=batch.edge_index,\n        edge_attr=batch.edge_attr,\n        batch=batch.batch,\n        mask=batch.mask,\n    )\n</code></pre>"},{"location":"tutorials/feature_reconstruction/#visualize-nodal-active-power-residuals","title":"Visualize Nodal Active Power Residuals","text":"<pre><code>visualize_error(batch, output, data_module.node_normalizers[0])\n</code></pre>"},{"location":"tutorials/feature_reconstruction/#visualize-the-state-reconstruction-capability-of-gridfm-v02-for-each-feature","title":"Visualize the state reconstruction capability of gridFM-v0.2 for each feature:","text":"<ul> <li>Active Power Demand (MW)</li> <li>Reactive Power Demand (MVar)</li> <li>Active Power Generated (MW)</li> <li>Reactive Power Generated (MVar)</li> <li>Voltage Magnitude (p.u.)</li> <li>Voltage Angle (degrees)</li> </ul> <pre><code>visualize_quantity_heatmap(\n    batch,\n    output,\n    PD,\n    \"Active Power Demand\",\n    \"MW\",\n    data_module.node_normalizers[0],\n)\n</code></pre> <pre><code>visualize_quantity_heatmap(\n    batch,\n    output,\n    QD,\n    \"Reactive Power Demand\",\n    \"MVar\",\n    data_module.node_normalizers[0],\n)\n</code></pre> <pre><code>visualize_quantity_heatmap(\n    batch,\n    output,\n    PG,\n    \"Active Power Generated\",\n    \"MW\",\n    data_module.node_normalizers[0],\n)\n</code></pre> <pre><code>visualize_quantity_heatmap(\n    batch,\n    output,\n    QG,\n    \"Reactive Power Generated\",\n    \"MVar\",\n    data_module.node_normalizers[0],\n)\n</code></pre> <pre><code>visualize_quantity_heatmap(\n    batch,\n    output,\n    VM,\n    \"Voltage magnitude\",\n    \"p.u.\",\n    data_module.node_normalizers[0],\n)\n</code></pre> <pre><code>visualize_quantity_heatmap(\n    batch,\n    output,\n    VA,\n    \"Voltage Angle\",\n    \"degrees\",\n    data_module.node_normalizers[0],\n)\n</code></pre>"}]}