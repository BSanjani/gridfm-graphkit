from gridfm_graphkit.io.param_handler import load_model, get_loss_function
from gridfm_graphkit.datasets.globals import (
    # Bus feature indices
    PD_H,
    QG_H,
    QD_H,
    VM_H,
    VA_H,
    MIN_QG_H,
    MAX_QG_H,
    # Output feature indices
    VM_OUT,
    VA_OUT,
    PG_OUT,
    QG_OUT,
    # Generator feature indices
    PG_H,
    C0_H,
    C1_H,
    C2_H,
    # Edge feature indices
    ANG_MIN,
    ANG_MAX,
    RATE_A,
)

from gridfm_graphkit.tasks.base_task import BaseTask
from gridfm_graphkit.io.registries import TASK_REGISTRY
from pytorch_lightning.utilities import rank_zero_only
import torch
import torch.nn.functional as F
from torch_scatter import scatter_add, scatter_mean, scatter_max
from gridfm_graphkit.models.utils import (
    ComputeBranchFlow,
    ComputeNodeInjection,
    ComputeNodeResiduals,
)
import matplotlib.pyplot as plt
import seaborn as sns
from lightning.pytorch.loggers import MLFlowLogger
import numpy as np
import os
import pandas as pd


class ReconstructionTask(BaseTask):
    """
    PyTorch Lightning task for node feature reconstruction on power grid graphs.

    This task wraps a GridFM model inside a LightningModule and defines the full
    training, validation, testing, and prediction logic. It is designed to
    reconstruct masked node features from graph-structured input data, using
    datasets and normalizers provided by `gridfm-graphkit`.

    Args:
        args (NestedNamespace): Experiment configuration. Expected fields include `training.batch_size`, `optimizer.*`, etc.
        data_normalizers (list): One normalizer per dataset to (de)normalize features.

    Attributes:
        model (torch.nn.Module): model loaded via `load_model`.
        loss_fn (callable): Loss function resolved from configuration.
        batch_size (int): Training batch size. From ``args.training.batch_size``
        data_normalizers (list): Dataset-wise feature normalizers.

    Methods:
        forward(x, pe, edge_index, edge_attr, batch, mask=None):
            Forward pass with optional feature masking.
        training_step(batch):
            One training step: computes loss, logs metrics, returns loss.
        validation_step(batch, batch_idx):
            One validation step: computes losses and logs metrics.

    """

    def __init__(self, args, data_normalizers):
        super().__init__(args, data_normalizers)
        self.model = load_model(args=args)
        self.loss_fn = get_loss_function(args)
        self.batch_size = int(args.training.batch_size)
        self.test_outputs = {i: [] for i in range(len(args.data.networks))}

    def forward(self, x_dict, edge_index_dict, edge_attr_dict, mask_dict):
        return self.model(x_dict, edge_index_dict, edge_attr_dict, mask_dict)

    def shared_step(self, batch):
        output = self.forward(
            x_dict=batch.x_dict,
            edge_index_dict=batch.edge_index_dict,
            edge_attr_dict=batch.edge_attr_dict,
            mask_dict=batch.mask_dict,
        )

        loss_dict = self.loss_fn(
            output,
            batch.y_dict,
            batch.edge_index_dict,
            batch.edge_attr_dict,
            batch.mask_dict,
            model=self.model,
        )
        return output, loss_dict

    def training_step(self, batch):
        _, loss_dict = self.shared_step(batch)
        current_lr = self.optimizer.param_groups[0]["lr"]
        metrics = {}
        metrics["Training Loss"] = loss_dict["loss"].detach()
        metrics["Learning Rate"] = current_lr
        for metric, value in metrics.items():
            self.log(
                metric,
                value,
                batch_size=batch.num_graphs,
                sync_dist=False,
                on_epoch=False,
                prog_bar=False,
                logger=True,
                on_step=True,
            )

        return loss_dict["loss"]

    def validation_step(self, batch, batch_idx):
        _, loss_dict = self.shared_step(batch)
        loss_dict["loss"] = loss_dict["loss"].detach()
        for metric, value in loss_dict.items():
            metric_name = f"Validation {metric}"
            self.log(
                metric_name,
                value,
                batch_size=batch.num_graphs,
                sync_dist=True,
                on_epoch=True,
                logger=True,
                on_step=False,
            )

        return loss_dict["loss"]

    @rank_zero_only
    def on_test_end(self):
        """Optional shared test end logic, like clearing stored outputs"""
        self.test_outputs.clear()


def residual_stats_by_type(residual, mask, bus_batch):
    residual_masked = residual[mask]
    batch_masked = bus_batch[mask]
    mean_res = scatter_mean(torch.abs(residual_masked), batch_masked, dim=0)
    max_res, _ = scatter_max(torch.abs(residual_masked), batch_masked, dim=0)
    return mean_res, max_res


def plot_residuals_histograms(outputs, dataset_name, plot_dir):
    """
    Plot mean/max residuals for P and Q, per bus type with consistent bins.
    """
    bus_types = ["REF", "PV", "PQ"]
    colors = ["#6baed6", "#fd8d3c", "#74c476"]  # PQ, PV, REF

    stats = [
        ("mean_residual_P", "Mean P Residual"),
        ("mean_residual_Q", "Mean Q Residual"),
        ("max_residual_P", "Max P Residual"),
        ("max_residual_Q", "Max Q Residual"),
    ]

    for stat_key, title in stats:
        # Gather all data first to compute common bin edges
        all_data = torch.cat(
            [
                torch.cat([d[f"{stat_key}_{bus_type}"] for d in outputs])
                for bus_type in bus_types
            ],
        ).numpy()

        # Define bins across the entire data range
        bins = np.linspace(all_data.min(), all_data.max(), 61)  # 30 bins of equal width

        plt.figure(figsize=(10, 6))
        for bus_type, color in zip(bus_types, colors):
            data = torch.cat([d[f"{stat_key}_{bus_type}"] for d in outputs]).numpy()
            plt.hist(data, bins=bins, alpha=0.6, label=bus_type, color=color)

        plt.title(f"{title} per Bus Type in {dataset_name}")
        plt.xlabel("Residual (MW or MVar)")
        plt.ylabel("Frequency")
        plt.legend(title="Bus Type")
        plt.grid(True, linestyle="--", alpha=0.5)
        plt.tight_layout()

        save_path = os.path.join(plot_dir, f"{stat_key}.png")
        plt.savefig(save_path, dpi=300)
        plt.close()


def plot_correlation_by_node_type(
    preds: torch.Tensor,
    targets: torch.Tensor,
    masks: dict,
    feature_labels: list,
    plot_dir: str,
    prefix: str = "",
    xlabel: str = "Target",
    ylabel: str = "Pred",
    qg_violation_mask: torch.Tensor = None,
):
    """
    Create correlation scatter plots per node type (PQ, PV, REF),
    and highlight Qg violations in red if a violation mask is provided.

    Args:
        preds (torch.Tensor): Predictions [N, F]
        targets (torch.Tensor): Targets [N, F]
        masks (dict): { "PQ": mask, "PV": mask, "REF": mask }
        feature_labels (list): Feature labels
        plot_dir (str): Directory to save plots
        prefix (str): Optional filename prefix
        qg_violation_mask (torch.BoolTensor): Global mask of Qg limit violations
    """

    os.makedirs(plot_dir, exist_ok=True)

    for node_type, mask in masks.items():
        if len(mask.shape) == 1:
            preds_masked = preds[mask]
            targets_masked = targets[mask]
        else:
            preds_masked = torch.where(mask, preds, 0)
            targets_masked = torch.where(mask, targets, 0)

        if preds_masked.numel() == 0:
            continue

        # ALSO mask Qg violations for this node type
        if qg_violation_mask is not None:
            qg_violation_mask_local = qg_violation_mask[mask].cpu().numpy()
        else:
            qg_violation_mask_local = None

        fig, axes = plt.subplots(2, 2, figsize=(15, 8))
        axes = axes.flatten()

        for i, (ax, label) in enumerate(zip(axes, feature_labels)):
            x = targets_masked[:, i].cpu().numpy().flatten()
            y = preds_masked[:, i].cpu().numpy().flatten()

            # --- normal scatter for all except Qg ---
            if label != "Qg" or qg_violation_mask_local is None:
                sns.scatterplot(x=x, y=y, s=6, alpha=0.4, ax=ax, edgecolor=None)
            else:
                # --- For Qg: split normal vs violating points ---
                normal_mask = ~qg_violation_mask_local
                viol_mask = qg_violation_mask_local

                # Normal (blue)
                sns.scatterplot(
                    x=x[normal_mask],
                    y=y[normal_mask],
                    s=6,
                    alpha=0.4,
                    ax=ax,
                    edgecolor=None,
                    label="Valid Qg",
                )

                # Violating (RED)
                sns.scatterplot(
                    x=x[viol_mask],
                    y=y[viol_mask],
                    s=8,
                    alpha=0.8,
                    ax=ax,
                    edgecolor="red",
                    color="red",
                    label="Qg violation",
                )

                ax.legend()

            # --- reference y=x line ---
            min_val = min(x.min(), y.min())
            max_val = max(x.max(), y.max())
            ax.plot(
                [min_val, max_val],
                [min_val, max_val],
                "k--",
                linewidth=1.0,
                alpha=0.7,
            )

            # --- R² correlation ---
            corr = np.corrcoef(x, y)[0, 1]
            if label != "Qg" or qg_violation_mask_local is None:
                ax.set_title(f"{node_type} – {label}\nR² = {corr**2:.3f}")
            else:
                num_violations = qg_violation_mask_local.sum().item()
                total_points = qg_violation_mask_local.shape[0]
                ax.set_title(
                    f"{node_type} – {label}\nR² = {corr**2:.3f} - {num_violations} violations out of {total_points} predictions",
                )
            ax.set_xlabel(xlabel)
            ax.set_ylabel(ylabel)

        plt.tight_layout()
        filename = f"{prefix}_correlation_{node_type}.png"
        plt.savefig(os.path.join(plot_dir, filename), dpi=300)
        plt.close(fig)


@TASK_REGISTRY.register("OptimalPowerFlow")
class OptimalPowerFlowTask(ReconstructionTask):
    """
    Concrete Optimal Power Flow task.
    Extends ReconstructionTask and adds OPF-specific metrics.
    """

    def __init__(self, args, data_normalizers):
        super().__init__(args, data_normalizers)

    def test_step(self, batch, batch_idx, dataloader_idx=0):
        output, loss_dict = self.shared_step(batch)
        dataset_name = self.args.data.networks[dataloader_idx]

        self.data_normalizers[dataloader_idx].inverse_transform(batch)
        self.data_normalizers[dataloader_idx].inverse_output(output)

        branch_flow_layer = ComputeBranchFlow()
        node_injection_layer = ComputeNodeInjection()
        node_residuals_layer = ComputeNodeResiduals()

        num_bus = batch.x_dict["bus"].size(0)
        bus_edge_index = batch.edge_index_dict[("bus", "connects", "bus")]
        bus_edge_attr = batch.edge_attr_dict[("bus", "connects", "bus")]
        _, gen_to_bus_index = batch.edge_index_dict[("gen", "connected_to", "bus")]

        mse_PG = F.mse_loss(
            output["gen"],
            batch.y_dict["gen"],
            reduction="none",
        ).mean(dim=0)
        c0 = batch.x_dict["gen"][:, C0_H]
        c1 = batch.x_dict["gen"][:, C1_H]
        c2 = batch.x_dict["gen"][:, C2_H]
        target_pg = batch.y_dict["gen"].squeeze()
        pred_pg = output["gen"].squeeze()
        gen_cost_gt = c0 + c1 * target_pg + c2 * target_pg**2
        gen_cost_pred = c0 + c1 * pred_pg + c2 * pred_pg**2

        gen_batch = batch.batch_dict["gen"]  # shape: [N_gen_total]

        cost_gt = scatter_add(gen_cost_gt, gen_batch, dim=0)
        cost_pred = scatter_add(gen_cost_pred, gen_batch, dim=0)

        optimality_gap = torch.mean(torch.abs((cost_pred - cost_gt) / cost_gt * 100))

        agg_gen_on_bus = scatter_add(
            batch.y_dict["gen"],
            gen_to_bus_index,
            dim=0,
            dim_size=num_bus,
        )
        # output_agg = torch.cat([batch.y_dict["bus"], agg_gen_on_bus], dim=1)
        target = torch.stack(
            [
                batch.y_dict["bus"][:, VM_H],
                batch.y_dict["bus"][:, VA_H],
                agg_gen_on_bus.squeeze(),
                batch.y_dict["bus"][:, QG_H],
            ],
            dim=1,
        )

        # UN-COMMENT THIS TO CHECK PBE ON GROUND TRUTH
        # output["bus"] = target

        Pft, Qft = branch_flow_layer(output["bus"], bus_edge_index, bus_edge_attr)
        # Compute branch termal limits violations
        Sft = torch.sqrt(Pft**2 + Qft**2)  # apparent power flow per branch
        branch_thermal_limits = bus_edge_attr[:, RATE_A]
        branch_thermal_excess = F.relu(Sft - branch_thermal_limits)

        num_edges = bus_edge_index.size(1)
        half_edges = num_edges // 2
        forward_excess = branch_thermal_excess[:half_edges]
        reverse_excess = branch_thermal_excess[half_edges:]

        mean_thermal_violation_forward = torch.mean(forward_excess)
        mean_thermal_violation_reverse = torch.mean(reverse_excess)

        # Compute branch angle difference violation
        angle_min = bus_edge_attr[:, ANG_MIN]
        angle_max = bus_edge_attr[:, ANG_MAX]

        bus_angles = output["bus"][:, VA_OUT]  # in degrees
        from_bus = bus_edge_index[0]
        to_bus = bus_edge_index[1]
        angle_diff = torch.abs(bus_angles[from_bus] - bus_angles[to_bus])

        angle_excess_low = F.relu(angle_min - angle_diff)  # violation if too small
        angle_excess_high = F.relu(angle_diff - angle_max)  # violation if too large
        branch_angle_violation_mean = (
            torch.mean(angle_excess_low + angle_excess_high) * 180.0 / torch.pi
        )

        P_in, Q_in = node_injection_layer(Pft, Qft, bus_edge_index, num_bus)
        residual_P, residual_Q = node_residuals_layer(
            P_in,
            Q_in,
            output["bus"],
            batch.x_dict["bus"],
        )

        # --- Qg limit violation mask ---
        Qg_pred = output["bus"][:, QG_OUT]
        Qg_max = batch.x_dict["bus"][:, MAX_QG_H]
        Qg_min = batch.x_dict["bus"][:, MIN_QG_H]

        mask_Qg_violation = (Qg_pred > Qg_max) | (Qg_pred < Qg_min)

        bus_batch = batch.batch_dict["bus"]  # shape: [num_bus_total]

        mask_PQ = batch.mask_dict["PQ"]  # PQ buses
        mask_PV = batch.mask_dict["PV"]  # PV buses
        mask_REF = batch.mask_dict["REF"]  # Reference buses

        Qg_over = F.relu(Qg_pred - Qg_max)  # amount above max limit
        Qg_under = F.relu(Qg_min - Qg_pred)  # amount below min limit
        Qg_violation_amount = Qg_over + Qg_under

        mean_Qg_violation_PV = Qg_violation_amount[mask_PV].mean()
        mean_Qg_violation_REF = Qg_violation_amount[mask_REF].mean()

        if self.args.verbose:
            mean_res_P_PQ, max_res_P_PQ = residual_stats_by_type(
                residual_P,
                mask_PQ,
                bus_batch,
            )
            mean_res_Q_PQ, max_res_Q_PQ = residual_stats_by_type(
                residual_Q,
                mask_PQ,
                bus_batch,
            )

            mean_res_P_PV, max_res_P_PV = residual_stats_by_type(
                residual_P,
                mask_PV,
                bus_batch,
            )
            mean_res_Q_PV, max_res_Q_PV = residual_stats_by_type(
                residual_Q,
                mask_PV,
                bus_batch,
            )

            mean_res_P_REF, max_res_P_REF = residual_stats_by_type(
                residual_P,
                mask_REF,
                bus_batch,
            )
            mean_res_Q_REF, max_res_Q_REF = residual_stats_by_type(
                residual_Q,
                mask_REF,
                bus_batch,
            )
            self.test_outputs[dataloader_idx].append(
                {
                    "dataset": dataset_name,
                    "pred": output["bus"].detach().cpu(),
                    "target": target.detach().cpu(),
                    "mask_PQ": mask_PQ.cpu(),
                    "mask_PV": mask_PV.cpu(),
                    "mask_REF": mask_REF.cpu(),
                    "cost_predicted": cost_pred.detach().cpu(),
                    "cost_ground_truth": cost_gt.detach().cpu(),
                    "mean_residual_P_PQ": mean_res_P_PQ.detach().cpu(),
                    "max_residual_P_PQ": max_res_P_PQ.detach().cpu(),
                    "mean_residual_Q_PQ": mean_res_Q_PQ.detach().cpu(),
                    "max_residual_Q_PQ": max_res_Q_PQ.detach().cpu(),
                    "mean_residual_P_PV": mean_res_P_PV.detach().cpu(),
                    "max_residual_P_PV": max_res_P_PV.detach().cpu(),
                    "mean_residual_Q_PV": mean_res_Q_PV.detach().cpu(),
                    "max_residual_Q_PV": max_res_Q_PV.detach().cpu(),
                    "mean_residual_P_REF": mean_res_P_REF.detach().cpu(),
                    "max_residual_P_REF": max_res_P_REF.detach().cpu(),
                    "mean_residual_Q_REF": mean_res_Q_REF.detach().cpu(),
                    "max_residual_Q_REF": max_res_Q_REF.detach().cpu(),
                    "mask_Qg_violation": mask_Qg_violation.detach().cpu(),
                },
            )

        final_residual_real_bus = torch.mean(torch.abs(residual_P))
        final_residual_imag_bus = torch.mean(torch.abs(residual_Q))

        loss_dict["Active Power Loss"] = final_residual_real_bus.detach()
        loss_dict["Reactive Power Loss"] = final_residual_imag_bus.detach()

        mse_PQ = F.mse_loss(
            output["bus"][mask_PQ],
            target[mask_PQ],
            reduction="none",
        )
        mse_PV = F.mse_loss(
            output["bus"][mask_PV],
            target[mask_PV],
            reduction="none",
        )
        mse_REF = F.mse_loss(
            output["bus"][mask_REF],
            target[mask_REF],
            reduction="none",
        )

        mse_PQ = mse_PQ.mean(dim=0)
        mse_PV = mse_PV.mean(dim=0)
        mse_REF = mse_REF.mean(dim=0)

        loss_dict["Opt gap"] = optimality_gap
        loss_dict["MSE PG"] = mse_PG[PG_H]

        loss_dict["Branch termal violation from"] = mean_thermal_violation_forward
        loss_dict["Branch termal violation to"] = mean_thermal_violation_reverse
        loss_dict["Branch voltage angle difference violations"] = (
            branch_angle_violation_mean
        )
        loss_dict["Mean Qg violation PV buses"] = mean_Qg_violation_PV
        loss_dict["Mean Qg violation REF buses"] = mean_Qg_violation_REF

        loss_dict["MSE PQ nodes - PG"] = mse_PQ[PG_OUT]
        loss_dict["MSE PV nodes - PG"] = mse_PV[PG_OUT]
        loss_dict["MSE REF nodes - PG"] = mse_REF[PG_OUT]

        loss_dict["MSE PQ nodes - QG"] = mse_PQ[QG_OUT]
        loss_dict["MSE PV nodes - QG"] = mse_PV[QG_OUT]
        loss_dict["MSE REF nodes - QG"] = mse_REF[QG_OUT]

        loss_dict["MSE PQ nodes - VM"] = mse_PQ[VM_OUT]
        loss_dict["MSE PV nodes - VM"] = mse_PV[VM_OUT]
        loss_dict["MSE REF nodes - VM"] = mse_REF[VM_OUT]

        loss_dict["MSE PQ nodes - VA"] = mse_PQ[VA_OUT]
        loss_dict["MSE PV nodes - VA"] = mse_PV[VA_OUT]
        loss_dict["MSE REF nodes - VA"] = mse_REF[VA_OUT]

        loss_dict["Test loss"] = loss_dict.pop("loss").detach()
        for metric, value in loss_dict.items():
            metric_name = f"{dataset_name}/{metric}"
            self.log(
                metric_name,
                value,
                batch_size=batch.num_graphs,
                add_dataloader_idx=False,
                sync_dist=True,
                logger=False,
            )
        return

    @rank_zero_only
    def on_test_end(self):
        if isinstance(self.logger, MLFlowLogger):
            artifact_dir = os.path.join(
                self.logger.save_dir,
                self.logger.experiment_id,
                self.logger.run_id,
                "artifacts",
            )
        else:
            artifact_dir = self.logger.save_dir

        final_metrics = self.trainer.callback_metrics
        grouped_metrics = {}

        for full_key, value in final_metrics.items():
            try:
                value = value.item()
            except AttributeError:
                pass

            if "/" in full_key:
                dataset_name, metric = full_key.split("/", 1)
                if dataset_name not in grouped_metrics:
                    grouped_metrics[dataset_name] = {}
                grouped_metrics[dataset_name][metric] = value

        for dataset, metrics in grouped_metrics.items():
            # RMSE metrics
            rmse_PQ = [
                metrics.get(f"MSE PQ nodes - {label}", float("nan")) ** 0.5
                for label in ["PG", "QG", "VM", "VA"]
            ]
            rmse_PV = [
                metrics.get(f"MSE PV nodes - {label}", float("nan")) ** 0.5
                for label in ["PG", "QG", "VM", "VA"]
            ]
            rmse_REF = [
                metrics.get(f"MSE REF nodes - {label}", float("nan")) ** 0.5
                for label in ["PG", "QG", "VM", "VA"]
            ]

            # Residuals and generator metrics
            avg_active_res = metrics.get("Active Power Loss", " ")
            avg_reactive_res = metrics.get("Reactive Power Loss", " ")
            rmse_gen = metrics.get("MSE PG", 0) ** 0.5
            optimality_gap = metrics.get("Opt gap", " ")
            branch_thermal_violation_from = metrics.get(
                "Branch termal violation from",
                " ",
            )
            branch_thermal_violation_to = metrics.get("Branch termal violation to", " ")
            branch_angle_violation = metrics.get(
                "Branch voltage angle difference violations",
                " ",
            )
            mean_qg_violation_PV_buses = metrics.get("Mean Qg violation PV buses", " ")
            mean_qg_violation_REF_buses = metrics.get(
                "Mean Qg violation REF buses",
                " ",
            )

            # --- Main RMSE metrics file ---
            data_main = {
                "Metric": ["RMSE-PQ", "RMSE-PV", "RMSE-REF"],
                "Pg (MW)": [rmse_PQ[0], rmse_PV[0], rmse_REF[0]],
                "Qg (MVar)": [rmse_PQ[1], rmse_PV[1], rmse_REF[1]],
                "Vm (p.u.)": [rmse_PQ[2], rmse_PV[2], rmse_REF[2]],
                "Va (radians)": [rmse_PQ[3], rmse_PV[3], rmse_REF[3]],
            }
            df_main = pd.DataFrame(data_main)

            # --- Residuals / generator metrics file ---
            data_residuals = {
                "Metric": [
                    "Avg. active res. (MW)",
                    "Avg. reactive res. (MVar)",
                    "RMSE PG generators (MW)",
                    "Mean optimality gap (%)",
                    "Mean branch termal violation from (MVA)",
                    "Mean branch termal violation to (MVA)",
                    "Mean branch angle difference violation (radians)",
                    "Mean Qg violation PV buses",
                    "Mean Qg violation REF buses",
                ],
                "Value": [
                    avg_active_res,
                    avg_reactive_res,
                    rmse_gen,
                    optimality_gap,
                    branch_thermal_violation_from,
                    branch_thermal_violation_to,
                    branch_angle_violation,
                    mean_qg_violation_PV_buses,
                    mean_qg_violation_REF_buses,
                ],
            }
            df_residuals = pd.DataFrame(data_residuals)

            # --- Save CSVs ---
            test_dir = os.path.join(artifact_dir, "test")
            os.makedirs(test_dir, exist_ok=True)

            main_csv_path = os.path.join(test_dir, f"{dataset}_RMSE.csv")
            residuals_csv_path = os.path.join(test_dir, f"{dataset}_metrics.csv")

            df_main.to_csv(main_csv_path, index=False)
            df_residuals.to_csv(residuals_csv_path, index=False)

        if self.args.verbose:
            for dataset_idx, outputs in self.test_outputs.items():
                dataset_name = self.args.data.networks[dataset_idx]

                plot_dir = os.path.join(artifact_dir, "test_plots", dataset_name)
                os.makedirs(plot_dir, exist_ok=True)

                # Concatenate predictions and targets across all batches
                all_preds = torch.cat([d["pred"] for d in outputs])
                all_targets = torch.cat([d["target"] for d in outputs])
                all_masks = {
                    "PQ": torch.cat([d["mask_PQ"] for d in outputs]),
                    "PV": torch.cat([d["mask_PV"] for d in outputs]),
                    "REF": torch.cat([d["mask_REF"] for d in outputs]),
                }
                all_cost_pred = torch.cat([d["cost_predicted"] for d in outputs])
                all_cost_ground_truth = torch.cat(
                    [d["cost_ground_truth"] for d in outputs],
                )

                # Convert to numpy for plotting
                y_pred = all_cost_pred.numpy()
                y_true = all_cost_ground_truth.numpy()

                # Compute correlation coefficient
                corr = np.corrcoef(y_true, y_pred)[0, 1]

                # Create scatter plot
                plt.figure(figsize=(6, 6))
                sns.scatterplot(x=y_true, y=y_pred, s=20, alpha=0.6)

                # Add y=x reference line
                min_val = min(y_true.min(), y_pred.min())
                max_val = max(y_true.max(), y_pred.max())
                plt.plot(
                    [min_val, max_val],
                    [min_val, max_val],
                    "k--",
                    linewidth=1.0,
                    alpha=0.7,
                )

                # Add correlation coefficient text
                plt.text(
                    0.05,
                    0.95,
                    f"R = {corr:.3f}",
                    transform=plt.gca().transAxes,
                    fontsize=12,
                    verticalalignment="top",
                    bbox=dict(facecolor="white", alpha=0.6),
                )

                plt.xlabel("Ground Truth Cost")
                plt.ylabel("Predicted Cost")
                plt.title(f"{dataset_name} – Predicted vs Ground Truth Cost")
                plt.tight_layout()
                plt.savefig(
                    os.path.join(plot_dir, f"{dataset_name}_objective.png"),
                    dpi=300,
                )
                plt.close()

                plot_residuals_histograms(outputs, dataset_name, plot_dir)

                plot_correlation_by_node_type(
                    preds=all_preds,
                    targets=all_targets,
                    masks=all_masks,
                    feature_labels=["Vm", "Va", "Pg", "Qg"],
                    plot_dir=plot_dir,
                    prefix=dataset_name,
                    qg_violation_mask=torch.cat(
                        [d["mask_Qg_violation"] for d in outputs],
                    ),
                )

        self.test_outputs.clear()

    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        raise NotImplementedError


@TASK_REGISTRY.register("PowerFlow")
class PowerFlowTask(ReconstructionTask):
    """
    Concrete Optimal Power Flow task.
    Extends ReconstructionTask and adds OPF-specific metrics.
    """

    def __init__(self, args, data_normalizers):
        super().__init__(args, data_normalizers)

    def test_step(self, batch, batch_idx, dataloader_idx=0):
        output, loss_dict = self.shared_step(batch)
        dataset_name = self.args.data.networks[dataloader_idx]

        self.data_normalizers[dataloader_idx].inverse_transform(batch)
        self.data_normalizers[dataloader_idx].inverse_output(output)

        branch_flow_layer = ComputeBranchFlow()
        node_injection_layer = ComputeNodeInjection()
        node_residuals_layer = ComputeNodeResiduals()

        num_bus = batch.x_dict["bus"].size(0)
        bus_edge_index = batch.edge_index_dict[("bus", "connects", "bus")]
        bus_edge_attr = batch.edge_attr_dict[("bus", "connects", "bus")]
        _, gen_to_bus_index = batch.edge_index_dict[("gen", "connected_to", "bus")]

        agg_gen_on_bus = scatter_add(
            batch.y_dict["gen"],
            gen_to_bus_index,
            dim=0,
            dim_size=num_bus,
        )
        # output_agg = torch.cat([batch.y_dict["bus"], agg_gen_on_bus], dim=1)
        target = torch.stack(
            [
                batch.y_dict["bus"][:, VM_H],
                batch.y_dict["bus"][:, VA_H],
                agg_gen_on_bus.squeeze(),
                batch.y_dict["bus"][:, QG_H],
            ],
            dim=1,
        )

        # UN-COMMENT THIS TO CHECK PBE ON GROUND TRUTH
        # output["bus"] = target

        Pft, Qft = branch_flow_layer(output["bus"], bus_edge_index, bus_edge_attr)
        P_in, Q_in = node_injection_layer(Pft, Qft, bus_edge_index, num_bus)
        residual_P, residual_Q = node_residuals_layer(
            P_in,
            Q_in,
            output["bus"],
            batch.x_dict["bus"],
        )

        bus_batch = batch.batch_dict["bus"]  # shape: [num_bus_total]

        mask_PQ = batch.mask_dict["PQ"]  # PQ buses
        mask_PV = batch.mask_dict["PV"]  # PV buses
        mask_REF = batch.mask_dict["REF"]  # Reference buses

        if self.args.verbose:
            mean_res_P_PQ, max_res_P_PQ = residual_stats_by_type(
                residual_P,
                mask_PQ,
                bus_batch,
            )
            mean_res_Q_PQ, max_res_Q_PQ = residual_stats_by_type(
                residual_Q,
                mask_PQ,
                bus_batch,
            )

            mean_res_P_PV, max_res_P_PV = residual_stats_by_type(
                residual_P,
                mask_PV,
                bus_batch,
            )
            mean_res_Q_PV, max_res_Q_PV = residual_stats_by_type(
                residual_Q,
                mask_PV,
                bus_batch,
            )

            mean_res_P_REF, max_res_P_REF = residual_stats_by_type(
                residual_P,
                mask_REF,
                bus_batch,
            )
            mean_res_Q_REF, max_res_Q_REF = residual_stats_by_type(
                residual_Q,
                mask_REF,
                bus_batch,
            )
            self.test_outputs[dataloader_idx].append(
                {
                    "dataset": dataset_name,
                    "pred": output["bus"].detach().cpu(),
                    "target": target.detach().cpu(),
                    "mask_PQ": mask_PQ.cpu(),
                    "mask_PV": mask_PV.cpu(),
                    "mask_REF": mask_REF.cpu(),
                    "mean_residual_P_PQ": mean_res_P_PQ.detach().cpu(),
                    "max_residual_P_PQ": max_res_P_PQ.detach().cpu(),
                    "mean_residual_Q_PQ": mean_res_Q_PQ.detach().cpu(),
                    "max_residual_Q_PQ": max_res_Q_PQ.detach().cpu(),
                    "mean_residual_P_PV": mean_res_P_PV.detach().cpu(),
                    "max_residual_P_PV": max_res_P_PV.detach().cpu(),
                    "mean_residual_Q_PV": mean_res_Q_PV.detach().cpu(),
                    "max_residual_Q_PV": max_res_Q_PV.detach().cpu(),
                    "mean_residual_P_REF": mean_res_P_REF.detach().cpu(),
                    "max_residual_P_REF": max_res_P_REF.detach().cpu(),
                    "mean_residual_Q_REF": mean_res_Q_REF.detach().cpu(),
                    "max_residual_Q_REF": max_res_Q_REF.detach().cpu(),
                },
            )

        final_residual_real_bus = torch.mean(torch.abs(residual_P))
        final_residual_imag_bus = torch.mean(torch.abs(residual_Q))

        loss_dict["Active Power Loss"] = final_residual_real_bus.detach()
        loss_dict["Reactive Power Loss"] = final_residual_imag_bus.detach()

        mse_PQ = F.mse_loss(
            output["bus"][mask_PQ],
            target[mask_PQ],
            reduction="none",
        )
        mse_PV = F.mse_loss(
            output["bus"][mask_PV],
            target[mask_PV],
            reduction="none",
        )
        mse_REF = F.mse_loss(
            output["bus"][mask_REF],
            target[mask_REF],
            reduction="none",
        )

        mse_PQ = mse_PQ.mean(dim=0)
        mse_PV = mse_PV.mean(dim=0)
        mse_REF = mse_REF.mean(dim=0)

        loss_dict["MSE PQ nodes - PG"] = mse_PQ[PG_OUT]
        loss_dict["MSE PV nodes - PG"] = mse_PV[PG_OUT]
        loss_dict["MSE REF nodes - PG"] = mse_REF[PG_OUT]

        loss_dict["MSE PQ nodes - QG"] = mse_PQ[QG_OUT]
        loss_dict["MSE PV nodes - QG"] = mse_PV[QG_OUT]
        loss_dict["MSE REF nodes - QG"] = mse_REF[QG_OUT]

        loss_dict["MSE PQ nodes - VM"] = mse_PQ[VM_OUT]
        loss_dict["MSE PV nodes - VM"] = mse_PV[VM_OUT]
        loss_dict["MSE REF nodes - VM"] = mse_REF[VM_OUT]

        loss_dict["MSE PQ nodes - VA"] = mse_PQ[VA_OUT]
        loss_dict["MSE PV nodes - VA"] = mse_PV[VA_OUT]
        loss_dict["MSE REF nodes - VA"] = mse_REF[VA_OUT]

        loss_dict["Test loss"] = loss_dict.pop("loss").detach()
        for metric, value in loss_dict.items():
            metric_name = f"{dataset_name}/{metric}"
            self.log(
                metric_name,
                value,
                batch_size=batch.num_graphs,
                add_dataloader_idx=False,
                sync_dist=True,
                logger=False,
            )
        return

    @rank_zero_only
    def on_test_end(self):
        if isinstance(self.logger, MLFlowLogger):
            artifact_dir = os.path.join(
                self.logger.save_dir,
                self.logger.experiment_id,
                self.logger.run_id,
                "artifacts",
            )
        else:
            artifact_dir = self.logger.save_dir

        final_metrics = self.trainer.callback_metrics
        grouped_metrics = {}

        for full_key, value in final_metrics.items():
            try:
                value = value.item()
            except AttributeError:
                pass

            if "/" in full_key:
                dataset_name, metric = full_key.split("/", 1)
                if dataset_name not in grouped_metrics:
                    grouped_metrics[dataset_name] = {}
                grouped_metrics[dataset_name][metric] = value

        for dataset, metrics in grouped_metrics.items():
            # RMSE metrics
            rmse_PQ = [
                metrics.get(f"MSE PQ nodes - {label}", float("nan")) ** 0.5
                for label in ["PG", "QG", "VM", "VA"]
            ]
            rmse_PV = [
                metrics.get(f"MSE PV nodes - {label}", float("nan")) ** 0.5
                for label in ["PG", "QG", "VM", "VA"]
            ]
            rmse_REF = [
                metrics.get(f"MSE REF nodes - {label}", float("nan")) ** 0.5
                for label in ["PG", "QG", "VM", "VA"]
            ]

            # Residuals and generator metrics
            avg_active_res = metrics.get("Active Power Loss", " ")
            avg_reactive_res = metrics.get("Reactive Power Loss", " ")

            # --- Main RMSE metrics file ---
            data_main = {
                "Metric": ["RMSE-PQ", "RMSE-PV", "RMSE-REF"],
                "Pg (MW)": [rmse_PQ[0], rmse_PV[0], rmse_REF[0]],
                "Qg (MVar)": [rmse_PQ[1], rmse_PV[1], rmse_REF[1]],
                "Vm (p.u.)": [rmse_PQ[2], rmse_PV[2], rmse_REF[2]],
                "Va (radians)": [rmse_PQ[3], rmse_PV[3], rmse_REF[3]],
            }
            df_main = pd.DataFrame(data_main)

            # --- Residuals / generator metrics file ---
            data_residuals = {
                "Metric": [
                    "Avg. active res. (MW)",
                    "Avg. reactive res. (MVar)",
                ],
                "Value": [avg_active_res, avg_reactive_res],
            }
            df_residuals = pd.DataFrame(data_residuals)

            # --- Save CSVs ---
            test_dir = os.path.join(artifact_dir, "test")
            os.makedirs(test_dir, exist_ok=True)

            main_csv_path = os.path.join(test_dir, f"{dataset}_RMSE.csv")
            residuals_csv_path = os.path.join(test_dir, f"{dataset}_metrics.csv")

            df_main.to_csv(main_csv_path, index=False)
            df_residuals.to_csv(residuals_csv_path, index=False)

        if self.args.verbose:
            for dataset_idx, outputs in self.test_outputs.items():
                dataset_name = self.args.data.networks[dataset_idx]

                plot_dir = os.path.join(artifact_dir, "test_plots", dataset_name)
                os.makedirs(plot_dir, exist_ok=True)

                # Concatenate predictions and targets across all batches
                all_preds = torch.cat([d["pred"] for d in outputs])
                all_targets = torch.cat([d["target"] for d in outputs])
                all_masks = {
                    "PQ": torch.cat([d["mask_PQ"] for d in outputs]),
                    "PV": torch.cat([d["mask_PV"] for d in outputs]),
                    "REF": torch.cat([d["mask_REF"] for d in outputs]),
                }

                plot_residuals_histograms(outputs, dataset_name, plot_dir)

                plot_correlation_by_node_type(
                    preds=all_preds,
                    targets=all_targets,
                    masks=all_masks,
                    feature_labels=["Vm", "Va", "Pg", "Qg"],
                    plot_dir=plot_dir,
                    prefix=dataset_name,
                )

        self.test_outputs.clear()

    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        raise NotImplementedError


@TASK_REGISTRY.register("StateEstimation")
class StateEstimationTask(ReconstructionTask):
    def __init__(self, args, data_normalizers):
        super().__init__(args, data_normalizers)

    # TODO: add custom test and predict steps
    def test_step(self, batch, batch_idx, dataloader_idx=0):
        output, loss_dict = self.shared_step(batch)
        dataset_name = self.args.data.networks[dataloader_idx]

        self.data_normalizers[dataloader_idx].inverse_transform(batch)
        self.data_normalizers[dataloader_idx].inverse_output(output)

        num_bus = batch.x_dict["bus"].size(0)
        _, gen_to_bus_index = batch.edge_index_dict[("gen", "connected_to", "bus")]

        agg_gen_on_bus = scatter_add(
            batch.y_dict["gen"],
            gen_to_bus_index,
            dim=0,
            dim_size=num_bus,
        )
        target = torch.stack(
            [
                batch.y_dict["bus"][:, VM_H],
                batch.y_dict["bus"][:, VA_H],
                agg_gen_on_bus.squeeze() - batch.y_dict["bus"][:, PD_H],
                batch.y_dict["bus"][:, QG_H] - batch.y_dict["bus"][:, QD_H],
            ],
            dim=1,
        )

        agg_meas_gen_on_bus = scatter_add(
            batch.x_dict["gen"][:, [PG_H]],
            gen_to_bus_index,
            dim=0,
            dim_size=num_bus,
        )

        # fig, ax = plt.subplots()
        # mask = batch.mask_dict['gen'][:, PG_H]
        # ax.hist((batch.x_dict['gen'][~mask, PG_H] - batch.y_dict['gen'][~mask, PG_H]).cpu().numpy(), bins=100)
        # fig.savefig('gen.png')

        # fig, ax = plt.subplots()
        # mask = batch.mask_dict['bus'][:, PD_H]
        # ax.hist((batch.x_dict['bus'][~mask, PD_H] - batch.y_dict['bus'][~mask, PD_H]).cpu().numpy(), bins=100)
        # fig.savefig('pd.png')

        measurements = torch.stack(
            [
                batch.x_dict["bus"][:, VM_H],
                batch.x_dict["bus"][:, VA_H],
                agg_meas_gen_on_bus.squeeze() - batch.x_dict["bus"][:, PD_H],
                batch.x_dict["bus"][:, QG_H] - batch.x_dict["bus"][:, QD_H],
            ],
            dim=1,
        )

        # fig, ax = plt.subplots()
        # mask = batch.mask_dict['bus'][:, PG_H]
        # ax.hist((agg_meas_gen_on_bus.squeeze()[~mask] - agg_gen_on_bus.squeeze()[~mask]).cpu().numpy(), bins=100)
        # fig.savefig('gen_to_bus.png')

        outliers_bus = batch.mask_dict["outliers_bus"]
        mask_bus = batch.mask_dict["bus"][:, : outliers_bus.size(1)]
        non_outliers_bus = torch.logical_and(~outliers_bus, ~mask_bus)
        masks = [outliers_bus, mask_bus, non_outliers_bus]
        for i, mask in enumerate(masks):
            new_mask = torch.zeros_like(target, dtype=bool)
            new_mask[:, VM_OUT] = mask[:, VM_H]
            new_mask[:, VA_OUT] = mask[:, VA_H]
            new_mask[:, PG_OUT] = mask[:, PD_H]
            new_mask[:, QG_OUT] = mask[:, QD_H]
            masks[i] = new_mask
        outliers_bus, mask_bus, non_outliers_bus = masks

        # fig, ax = plt.subplots()
        # ax.hist((measurements[~mask_bus[:, PG_OUT], PG_OUT] - target[~mask_bus[:, PG_OUT], PG_OUT]).cpu().numpy(), bins=100)
        # fig.savefig('p_inj.png')
        # assert False

        self.test_outputs[dataloader_idx].append(
            {
                "dataset": dataset_name,
                "pred": output["bus"].detach().cpu(),
                "target": target.detach().cpu(),
                "measurement": measurements.cpu(),
                "mask_bus": mask_bus.detach().cpu(),
                "outliers_bus": outliers_bus.detach().cpu(),
                "non_outliers_bus": non_outliers_bus.detach().cpu(),
            },
        )

    @rank_zero_only
    def on_test_end(self):
        if isinstance(self.logger, MLFlowLogger):
            artifact_dir = os.path.join(
                self.logger.save_dir,
                self.logger.experiment_id,
                self.logger.run_id,
                "artifacts",
            )
        else:
            artifact_dir = self.logger.save_dir

        if self.args.verbose:
            for dataset_idx, outputs in self.test_outputs.items():
                dataset_name = self.args.data.networks[dataset_idx]

                plot_dir = os.path.join(artifact_dir, "test_plots", dataset_name)
                os.makedirs(plot_dir, exist_ok=True)

                # Concatenate predictions and targets across all batches
                all_preds = torch.cat([d["pred"] for d in outputs])
                all_targets = torch.cat([d["target"] for d in outputs])
                all_measurements = torch.cat([d["measurement"] for d in outputs])

                all_masks = {
                    m: torch.cat([d[m] for d in outputs])
                    for m in ["mask_bus", "outliers_bus", "non_outliers_bus"]
                }

                plot_correlation_by_node_type(
                    preds=all_preds,
                    targets=all_targets,
                    masks=all_masks,
                    feature_labels=["Vm", "Va", "Pg", "Qg"],
                    plot_dir=plot_dir,
                    prefix=dataset_name + "_pred_vs_target_",
                    xlabel="Target",
                    ylabel="Pred",
                )

                plot_correlation_by_node_type(
                    preds=all_preds,
                    targets=all_measurements,
                    masks=all_masks,
                    feature_labels=["Vm", "Va", "Pg", "Qg"],
                    plot_dir=plot_dir,
                    prefix=dataset_name + "_pred_vs_measured_",
                    xlabel="Measured",
                    ylabel="Pred",
                )

                plot_correlation_by_node_type(
                    preds=all_measurements,
                    targets=all_targets,
                    masks=all_masks,
                    feature_labels=["Vm", "Va", "Pg", "Qg"],
                    plot_dir=plot_dir,
                    prefix=dataset_name + "_measured_vs_target_",
                    xlabel="Target",
                    ylabel="Measured",
                )

        self.test_outputs.clear()

    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        pass
